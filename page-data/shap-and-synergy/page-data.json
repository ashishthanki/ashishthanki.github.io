{
    "componentChunkName": "component---themes-advanced-src-templates-post-query-ts",
    "path": "/shap-and-synergy",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"SHAP and Synergy\",\n  \"cover\": \"images/synergy/redundancy_matrix.png\",\n  \"coverAlt\": \"Redundancy Matrix produced by BCG Gamma\",\n  \"description\": \"SHAP and Synergy matrix will be an important part of a machine learning pipeline.\",\n  \"datePublished\": \"2023-02-10\",\n  \"dateModified\": \"2023-02-10\",\n  \"category\": \"Interpretability\",\n  \"tags\": [\"Machine Learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Understanding how and why a model produces an output is becoming an increasingly important stage when building a machine learning solution. Expanding on my \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"category/Interpretability\"\n  }, \"explainability\"), \" blogs this blog takes a brief look at Synergy, Redundancy and Independence which uses \\\"\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"sophisticated model inspection and model-based simulation to enable better explanations of supervised machine learning models.\"), \"\\\"\"), mdx(\"p\", null, \"Not only are data scientists required to produce an \\u201Caccurate\\u201D and \\u201Creliable\\u201D model, we are also required to unravel our models:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Diagnostic\"), \": ensuring the model is not a victim to data leakage and has properly generalised to the training data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Validation\"), \": check the plausibility of the relationships between the features and target variables. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Feature Selection\"), \": prune redundant features while keeping features that have \\u201Csynergistic\\u201D features.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Fairness and compliance\"), \": avoid discrimination, bias, or other regulatory requirements. Yes, there are even GDPR requirements (some explanatory frameworks are achieving \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1906.09293\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"this\"), \" compliancy).\")), mdx(\"p\", null, \"Comparing coefficients of linear regression models, activations of neurons or tracking the number of times a feature is used within a random forest model still provides a limited insight into their decision making process.\"), mdx(\"h2\", null, \"SHAP\"), mdx(\"p\", null, \"The most popular explanatory framework is Shapely Additive exPlanation (SHAP) but this also fails to unpack an arbitrary \\u201Cblack box\\u201D machine learning model. It uses local contributions of feature (or features) but is not designed for global relationships amongst features.\"), mdx(\"p\", null, \"With SHAP we are not able to answer (the more challenging) questions about features:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Given a group of features, would there be less impact if their counterparts were absent?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Can features be substituted with little impact to the model performance?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"What are the effects of a group of features on model performance?\")), mdx(\"h2\", null, \"Synergy, Redundancy and Independence\"), mdx(\"p\", null, \"Given some features, we can take SHAP values and represent them as vectors. Following \\u201Cdecomposition\\u201D, we can split them into multiple subvectors. The driver for this is to capture information lost by Shapely values. By decomposing the values and given these subvectors, we can build three types of relationships for the features:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Synergy\"), \": given feature x1 relative to another feature x2 quantifies the degree to which predictive contributions of x1 rely on information from x2. For example, we can think of coordinates, longitude and latitude, would be related features and are used synergistically to predict - one cannot determine the outcome without the other.\"), mdx(\"p\", null, \"Synergy is expressed as a percentage where 0% is full autonomy to 100% is full synergy and measures the global information of two interacting features contribute to the model predictions.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Redundancy\"), \": given two features x1 and x2, x2 might be redundant because x1 already contains the information present in x2. For example, height in inches and height in centimeters are highly redundant features.\"), mdx(\"p\", null, \"Redundancy is expressed as a percentage where 0% is full uniqueness to 100% is full redundancy. The redundancy is asymmetric, one feature may be more redundant than the other. For example, house size and number of rooms. The number of rooms is more redundant than house size because house size can better compensate for the absence of number of rooms. Removing house size will be detrimental to the model performance.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Independence\"), \": features x1 and x2 are neither synergistic or redundant.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/0e0c4/synergy_vectors.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"57.8125%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABnElEQVQoz31S2W7CMBDk/38D8cIPVPSxEg9IIEQFCDVRGlAcziRALp+Zyk5M0zbF1srrtXc8491eVVXQQymFyWQC3/Nxvp3hXT7reKVg79j12ejZS1mWYTAY4HX0io37gdMtqUH0bAFaa+/bjz0AKaXYbncghCDNclChoLiEFKqTXXvf9nvtQJ7niOPY+AXnqITC4f2AMik7Gd5pioxl3QyFEAiCwPhSKZzzvAaOC9A7/cWkglQVruUdBS+6AcMwNLIt4Mh14TVs9T/aoc+YlKZY/0pOkgRRFD0OddK1LFEKUcfEt1Qdf3EcpIz9AHsw5JybQrSDQinkTYLgAovVAhktIJvzuChA7WNNy2kzgGFIHlLrYC0w57xupyLDercGU8xkKyuvYWyBLKHe9Xr705yXS4w4TSE4h3fzsE23T5t5uVzCdd2aYRTtoS0IVjidd7icQowXbxjPJ5hNpzgcD1h7a5CQYL/fm+JZ01+l22w4HKLf78NxHP2HzLQMITOQcK6VgEkKLriRwxiD5NKsXaZrsNlscDwe4fs+vgC4W6IfHYScEwAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Geometric interpretation of synergy, redundancy and independence of feature xi relative to feature xj. The shap vectors are decomposed to produce S-R-I values.\",\n    \"title\": \"Geometric interpretation of synergy, redundancy and independence of feature xi relative to feature xj. The shap vectors are decomposed to produce S-R-I values.\",\n    \"src\": \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/e5715/synergy_vectors.png\",\n    \"srcSet\": [\"/static/f2a353cc01013c4ba10fea1cdedd3ff0/8514f/synergy_vectors.png 192w\", \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/804b2/synergy_vectors.png 384w\", \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/e5715/synergy_vectors.png 768w\", \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/4ad3a/synergy_vectors.png 1152w\", \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/71c1d/synergy_vectors.png 1536w\", \"/static/f2a353cc01013c4ba10fea1cdedd3ff0/0e0c4/synergy_vectors.png 1884w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Geometric interpretation of synergy, redundancy and independence of feature xi relative to feature xj. The shap vectors are decomposed to produce S-R-I values.\"), \"\\n  \"), mdx(\"p\", null, \"All three of the relationships are expressed as percentages and features add up to 100%. Furthermore, feature relationships are not symmetrical meaning if feature x1 is synergistic with a group of others the reverse may not necessarily be true.\"), mdx(\"p\", null, \"The details of SHAP and S-R-I is based on vectors and information provided within the vector space - for example, the vector angles between two vectors and decompose vectors into orthogonal components.\"), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"Model explainability is a growing area of research and will only become more important as machine learning models are used to make decisions. Frameworks like SHAP and LIME are locally comprehensive but still lack the global explanations that Synergy, Redundancy and Independence (S-R-I) decomposition of SHAP vectors offers. I am sure there will be new concepts to learn as this field grows, watch this space!\"), mdx(\"h4\", null, \"Further Reading:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2107.12436\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://bcg-gamma.github.io/facet/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"BCG Gamma Library\"), \" implements the SRI approach with other useful tools to enable better model explanations.\"))));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":3,"excerpt":"Understanding how and why a model produces an output is becoming an increasingly important stage when building a machine learning solution…","frontmatter":{"title":"SHAP and Synergy","description":"SHAP and Synergy matrix will be an important part of a machine learning pipeline.","cover":{"publicURL":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/redundancy_matrix.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEEklEQVQ4y0WU3W+TZRjG+/cYPVET48Y2Bmu7te/3V9+v9d26FQJBUQSREIEQEoEYD2CRD8cgLAjZ6ApBYFmAaNCDGYPxwATQRaLAWrqOdd0HyLqf9u2MB3fuPPfBL8/1XNf9RF6trHCb37lWfED+xwdcn7jN+M0JRnNXOHlqkKGzwwwOnefcuWFu3hjnxlqNj0+QG7vK1G9TrALfVh7ybKZEZOnFC6SX53jn7mne/HiIVtGgvT3O+qhEtFOhI66wsVMlGhPZUJ+3xVjfFiUWTfLa62/x9fAlakDf4zwLL5eJVJcWSV24TuLuZdT5S5j9WYwuEyPlozh9qE4fupbC0B0Mw8UwvbA7TkBry0byo/kQuP3RnbBHqgtVtN3HiB87gj75OdrAJyi7tiKqPmIqg2QFyLKJolgosoWkpJBUBz2VpqmpjdxILgRt/vU7iuUykdnZWXwvTTIqInVoKDNnkX/6kngig5gKkE0fSfdRVHsNaCMaaTQ3S3NzO2MjOVaATT9PUl1eJrKwsIDfu4WkYqOYHtLOHUhDh1AeniD+6SGi9keIqV5k3UfWHESrJyzd7Wdda5Sxy/kQmJ38ZU1ytYrXs4WE4iBbaZKd3Qj7diM+Ok/nhZPEDh4l3rcD0elH0j2EOtBMo9u9NLf8D+y9c68huVKZx+vup0t2SDpZZMMl4W1hffYw0tQg0h/nad31FZ19uxDM7lC+rNqYlkdzUztjow3JmW++p7q03LihG2wmIVkouousuaEZnentxI8P0HVjGOH+BYQDexGEAFmzQ4OsOnBdR3jDV3XgyK0QHKlUKrjpTSQEHVXSUTQH2fARzDQd/QeJfTGA/OQUyuAh5K3vI2smqloH+jSt6yC3Jjk4c4Vn5Vkic3NzpNMZREENXVRkA1mxwmiIdi9JN0tS24x07wRqcQilTURKGBhuD++2xMnlrjaAAxepLi4SmZ+fJ92dQRC0RjTUVMNNoxvJDJDMNKLioex7D+3MXrRbR5D3bEMT3IbLuSuh5ODoWVZqqw3JQdCPJJloho9meChmN7KdQbWC8KxpJnqHgp7pQV25hDa0H8Pupa0jTj5/tQHcf5zZ53ONHNqWTywmkhR04rJDXPGIKy5dokFS0BCSCgnJoEu36dqgk5g4jFK7yNtvtDAyPNrY5T1HeD5Xabjs+xkSCRVJMpBUG1F1wqBLshWunSjqjQ2pP0VcRz6wHTP3GU2nP2Ds/g+sLP1Nduc+6j9XpFwuk920LXTNtjx8N8BzA6xUN64T4Lo9WKk0jt+P4/Xh+QG25BH0biW2Osw1pqjNLvHh7r0sLCwSASiVZigUihSLRf56/DjshUKBp0+nKRaKTE8XePLkKdP/zUoliuUSf84UeEWN1VqNUqnE4r8u/wO4cGdzGfQFOQAAAABJRU5ErkJggg=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/0a3e8/redundancy_matrix.png","srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/a340d/redundancy_matrix.png 141w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/2c509/redundancy_matrix.png 282w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/0a3e8/redundancy_matrix.png 564w","sizes":"(min-width: 564px) 564px, 100vw"},"sources":[{"srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/46dd4/redundancy_matrix.avif 141w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/9bb51/redundancy_matrix.avif 282w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/d76c5/redundancy_matrix.avif 564w","type":"image/avif","sizes":"(min-width: 564px) 564px, 100vw"},{"srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/4c277/redundancy_matrix.webp 141w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/92316/redundancy_matrix.webp 282w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/cc416/redundancy_matrix.webp 564w","type":"image/webp","sizes":"(min-width: 564px) 564px, 100vw"}]},"width":564,"height":479.00000000000006}}},"coverAlt":"Redundancy Matrix produced by BCG Gamma","datePublished":"2023-02-10","dateModified":"2023-02-10","category":"Interpretability","tags":["Machine Learning"]},"fields":{"slug":"/shap-and-synergy","route":"/shap-and-synergy","pathName":"/shap-and-synergy","url":"https://ashishthanki.github.io/shap-and-synergy"},"internal":{"content":"---\ntitle: \"SHAP and Synergy\"\ncover: images/synergy/redundancy_matrix.png\ncoverAlt: \"Redundancy Matrix produced by BCG Gamma\"\ndescription: \"SHAP and Synergy matrix will be an important part of a machine learning pipeline.\"\ndatePublished: \"2023-02-10\"\ndateModified: \"2023-02-10\"\ncategory: \"Interpretability\"\ntags:\n  - Machine Learning\n---\n\n\nUnderstanding how and why a model produces an output is becoming an increasingly important stage when building a machine learning solution. Expanding on my [explainability](category/Interpretability) blogs this blog takes a brief look at Synergy, Redundancy and Independence which uses \"*sophisticated model inspection and model-based simulation to enable better explanations of supervised machine learning models.*\"\n\nNot only are data scientists required to produce an “accurate” and “reliable” model, we are also required to unravel our models:\n\n- **Diagnostic**: ensuring the model is not a victim to data leakage and has properly generalised to the training data.\n- **Validation**: check the plausibility of the relationships between the features and target variables. \n- **Feature Selection**: prune redundant features while keeping features that have “synergistic” features.\n- **Fairness and compliance**: avoid discrimination, bias, or other regulatory requirements. Yes, there are even GDPR requirements (some explanatory frameworks are achieving [this](https://arxiv.org/abs/1906.09293) compliancy).\n\nComparing coefficients of linear regression models, activations of neurons or tracking the number of times a feature is used within a random forest model still provides a limited insight into their decision making process.\n\n## SHAP\n\nThe most popular explanatory framework is Shapely Additive exPlanation (SHAP) but this also fails to unpack an arbitrary “black box” machine learning model. It uses local contributions of feature (or features) but is not designed for global relationships amongst features.\n\nWith SHAP we are not able to answer (the more challenging) questions about features:\n\n- Given a group of features, would there be less impact if their counterparts were absent?\n- Can features be substituted with little impact to the model performance?\n- What are the effects of a group of features on model performance?\n\n## Synergy, Redundancy and Independence\n\nGiven some features, we can take SHAP values and represent them as vectors. Following “decomposition”, we can split them into multiple subvectors. The driver for this is to capture information lost by Shapely values. By decomposing the values and given these subvectors, we can build three types of relationships for the features:\n\n**Synergy**: given feature x1 relative to another feature x2 quantifies the degree to which predictive contributions of x1 rely on information from x2. For example, we can think of coordinates, longitude and latitude, would be related features and are used synergistically to predict - one cannot determine the outcome without the other.\n\nSynergy is expressed as a percentage where 0% is full autonomy to 100% is full synergy and measures the global information of two interacting features contribute to the model predictions.\n\n**Redundancy**: given two features x1 and x2, x2 might be redundant because x1 already contains the information present in x2. For example, height in inches and height in centimeters are highly redundant features.\n\nRedundancy is expressed as a percentage where 0% is full uniqueness to 100% is full redundancy. The redundancy is asymmetric, one feature may be more redundant than the other. For example, house size and number of rooms. The number of rooms is more redundant than house size because house size can better compensate for the absence of number of rooms. Removing house size will be detrimental to the model performance.\n\n**Independence**: features x1 and x2 are neither synergistic or redundant.\n\n\n![Geometric interpretation of synergy, redundancy and independence of feature xi relative to feature xj. The shap vectors are decomposed to produce S-R-I values.](/images/synergy/synergy_vectors.png)\n\nAll three of the relationships are expressed as percentages and features add up to 100%. Furthermore, feature relationships are not symmetrical meaning if feature x1 is synergistic with a group of others the reverse may not necessarily be true.\n\nThe details of SHAP and S-R-I is based on vectors and information provided within the vector space - for example, the vector angles between two vectors and decompose vectors into orthogonal components.\n\n\n## Conclusion\n\nModel explainability is a growing area of research and will only become more important as machine learning models are used to make decisions. Frameworks like SHAP and LIME are locally comprehensive but still lack the global explanations that Synergy, Redundancy and Independence (S-R-I) decomposition of SHAP vectors offers. I am sure there will be new concepts to learn as this field grows, watch this space!\n\n\n#### Further Reading:\n\n- [Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition](https://arxiv.org/abs/2107.12436)\n\n- [BCG Gamma Library](https://bcg-gamma.github.io/facet/) implements the SRI approach with other useful tools to enable better model explanations.\n"}}},"pageContext":{"slug":"/shap-and-synergy","nexttitle":"Sequences","nextslug":"/sequences","prevtitle":"AWS Infrastructure","prevslug":"/aws-infrastructure","relatedPosts":[{"title":"Interpreting ML Models Part 2","description":"Second part in understanding ML Models.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAGkhnAX/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEDERMU/9oACAEBAAEFAnM2bUdDP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CjRWYf//EABoQAAICAwAAAAAAAAAAAAAAAAABIVExYbH/2gAIAQEAAT8hzaQ7XSkq4P/aAAwDAQACAAMAAAAQDC//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEUH/2gAIAQMBAT8Q2I//xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QGl1d/8QAHBABAAICAwEAAAAAAAAAAAAAAQAhEZExUWGx/9oACAEBAAE/EDEe0jSbichdpTUAoMemXyf/2Q=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg","srcSet":"/static/b57872d67fc00d945468ce891f38f187/2ab11/cover_6.jpg 230w,\n/static/b57872d67fc00d945468ce891f38f187/16c5c/cover_6.jpg 460w,\n/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg 920w,\n/static/b57872d67fc00d945468ce891f38f187/3f865/cover_6.jpg 1840w","sizes":"(min-width: 920px) 920px, 100vw"},"sources":[{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/ee20b/cover_6.avif 230w,\n/static/b57872d67fc00d945468ce891f38f187/5c720/cover_6.avif 460w,\n/static/b57872d67fc00d945468ce891f38f187/991c3/cover_6.avif 920w,\n/static/b57872d67fc00d945468ce891f38f187/b7eae/cover_6.avif 1840w","type":"image/avif","sizes":"(min-width: 920px) 920px, 100vw"},{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/1d6f2/cover_6.webp 230w,\n/static/b57872d67fc00d945468ce891f38f187/f0b6f/cover_6.webp 460w,\n/static/b57872d67fc00d945468ce891f38f187/d54e9/cover_6.webp 920w,\n/static/b57872d67fc00d945468ce891f38f187/382d3/cover_6.webp 1840w","type":"image/webp","sizes":"(min-width: 920px) 920px, 100vw"}]},"width":920,"height":368},"coverImageAlt":"A view of mountains.","datePublished":"2021-09-26T00:00:00.000Z","dateModified":"2021-09-26T00:00:00.000Z","category":"Interpretability","tags":["Machine Learning"],"excerpt":"Introduction Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory…","timeToRead":7,"slug":"/interpreting-ml-models-part-2","route":"/interpreting-ml-models-part-2","pathName":"/interpreting-ml-models-part-2","url":"https://ashishthanki.github.io/interpreting-ml-models-part-2"},{"title":"Interpreting ML Models","description":"First part in understanding ML models.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAGkhnAX/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEDERMU/9oACAEBAAEFAnM2bUdDP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CjRWYf//EABoQAAICAwAAAAAAAAAAAAAAAAABIVExYbH/2gAIAQEAAT8hzaQ7XSkq4P/aAAwDAQACAAMAAAAQDC//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEUH/2gAIAQMBAT8Q2I//xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QGl1d/8QAHBABAAICAwEAAAAAAAAAAAAAAQAhEZExUWGx/9oACAEBAAE/EDEe0jSbichdpTUAoMemXyf/2Q=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg","srcSet":"/static/b57872d67fc00d945468ce891f38f187/2ab11/cover_6.jpg 230w,\n/static/b57872d67fc00d945468ce891f38f187/16c5c/cover_6.jpg 460w,\n/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg 920w,\n/static/b57872d67fc00d945468ce891f38f187/3f865/cover_6.jpg 1840w","sizes":"(min-width: 920px) 920px, 100vw"},"sources":[{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/ee20b/cover_6.avif 230w,\n/static/b57872d67fc00d945468ce891f38f187/5c720/cover_6.avif 460w,\n/static/b57872d67fc00d945468ce891f38f187/991c3/cover_6.avif 920w,\n/static/b57872d67fc00d945468ce891f38f187/b7eae/cover_6.avif 1840w","type":"image/avif","sizes":"(min-width: 920px) 920px, 100vw"},{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/1d6f2/cover_6.webp 230w,\n/static/b57872d67fc00d945468ce891f38f187/f0b6f/cover_6.webp 460w,\n/static/b57872d67fc00d945468ce891f38f187/d54e9/cover_6.webp 920w,\n/static/b57872d67fc00d945468ce891f38f187/382d3/cover_6.webp 1840w","type":"image/webp","sizes":"(min-width: 920px) 920px, 100vw"}]},"width":920,"height":368},"coverImageAlt":"A view of mountains","datePublished":"2021-07-02T00:00:00.000Z","dateModified":"2021-07-02T00:00:00.000Z","category":"Interpretability","tags":["Machine Learning"],"excerpt":"Introduction Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory…","timeToRead":5,"slug":"/interpreting-ml-models","route":"/interpreting-ml-models","pathName":"interpreting-ml-models","url":"https://ashishthanki.github.io/interpreting-ml-models"}]}},
    "staticQueryHashes": ["3661114550"]}