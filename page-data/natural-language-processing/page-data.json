{
    "componentChunkName": "component---themes-advanced-src-templates-post-query-ts",
    "path": "/natural-language-processing",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Natural Language Processing\",\n  \"cover\": \"images/cover_5.jpeg\",\n  \"coverAlt\": \"This Image\",\n  \"description\": \"Natural Language Processing is an extensive topic and this blog tries to summarise it.\",\n  \"datePublished\": \"2021-11-09\",\n  \"dateModified\": \"2021-11-09\",\n  \"category\": \"NLP\",\n  \"tags\": [\"NLP\", \"TensorFlow\", \"HuggingFace\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Natural Language Processing (NLP) is a bit of a buzzword nowadays with job posts asking for 10 years of experience on the BERT model that has only been around since 2018. NLP is still an active area of research and is changing very quickly every year. With new architectures and layers being wrapped together for the finest of improvements. This blog gives an overview of NLP and provides resources to allow you to explore further.\"), mdx(\"h2\", null, \"Natural Language Processing\"), mdx(\"p\", null, \"Natural language processing is the interaction between computers and human languages. Text data ultimately needs to be processed in such a way that computers can understand and analyse in order to gain hidden insight about the data. \"), mdx(\"p\", null, \"NLP unlocks the door for solving many problems, such as, automatic summarization, translation, sentiment analysis, speech recognition and topic segmentation. Text data is all around us, in reports, books, comments, blog posts, news articles, emails and more. Having to look at this data manually can be troublesome. \"), mdx(\"p\", null, \"A good example of using NLP is interpreting surveys. Imagine, you carry out a survey asking individuals from various countries how they feel about their national government. Going through individual descriptions would take a long time. Instead, using NLP we can classify each comment into negative, neutral or positive. Grouping the comments into countries so we can then perform data aggregation. Giving a clearer picture on how individuals feel about their government. This is just the tip of what NLP can offer and is called Sentiment Analysis, probably one of the widely known applications.\"), mdx(\"p\", null, \"Without taking you back to primary/elementary school, we know that words can be \\u201Cclassed\\u201D under nouns, verbs, adjectives and adverbs. These classes are known as lexical categories and form the foundation for many NLP tasks so read up on them (here)\", \"[https://www.bbc.co.uk/bitesize/guides/zx3gr82/revision/5]\", \" and come back! If you have a background in tabular data, you can think of these lexical categories as features. The more useful the feature that we can extract about the data the more useful the NLP model will be at its task.\"), mdx(\"h2\", null, \"Cleaning text data types\"), mdx(\"p\", null, \"In my opinion, the most challenging part about working with text data is the data cleaning and processing stage. Most models will be off-the-shelf from TensorFlow Hub. It is unlikely that you'll be training your own NLP model. It is, however, likely that you'll have to preprocess your input data so that it is in the correct format (and shape!) for the selected model. Rather than providing the inidividuals steps for a single model, below are a few popular data processing techniques. Remember, the aim of these techniques are to provide additional features that the model can learn from.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"598px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/aec58ab4f31ac00ac6e54fe45107e89d/0c69d/text-pipeline.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"36.97916666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSklEQVQY012QXW+bMBhG+f8/qhfd1FbVkjYpGR/GgA2GGGNjU9s4NOGt2knTtOf6PBfnRACAGnW9bQAwzX6a/ajtElbrL2r2avZuuVgfpHFSO7dc4J9Fs7vc7QlX1vpQt8PbKa2brmSSdGOaoRzXdSeb8yQnIyfDBm19MHbR734Ja7Q7ZrhAqCgPqM/L5vnpCaPsd8lT3B5f96c4RlSUTKbJqS4LzNQuxvc/H+5/PCaYRQlui5pVTY+oaHrxstuRCudUEDbkyQnlqOUaU344vGZZWtCB9Orhef/469gJE91u28d1u942H1ahZmNmaZeaGzrotMCIsnowlOsQgnWedBIA4mp4w/zL+Y/69tULUjJmRKj2HNTkuBhbPhIWuFj0nJCxYmr9uAKAC6sL6//nhIz0rEEOW8ugodB3UFdACOgpJrLh+pvc/tb+BF47gRMeVrhNAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Text Cleaning Pipeline\",\n    \"title\": \"Text Cleaning Pipeline\",\n    \"src\": \"/static/aec58ab4f31ac00ac6e54fe45107e89d/0c69d/text-pipeline.png\",\n    \"srcSet\": [\"/static/aec58ab4f31ac00ac6e54fe45107e89d/8514f/text-pipeline.png 192w\", \"/static/aec58ab4f31ac00ac6e54fe45107e89d/804b2/text-pipeline.png 384w\", \"/static/aec58ab4f31ac00ac6e54fe45107e89d/0c69d/text-pipeline.png 598w\"],\n    \"sizes\": \"(max-width: 598px) 100vw, 598px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Text Cleaning Pipeline\"), \"\\n  \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Removing StopWords\"), \": words that add no value to the interpretation of the data should be removed. For example, words like \\u201Can\\u201D, \\u201Cthe\\u201D, \\u201Ca\\u201D, \\u201Cbe\\u201D etc. create noise while modelling. \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tokenization\"), \": break down the unstructured text data into chunks of information so it can be counted as discrete elements. For example, split a sentence into individual words. The BERT model has a specific tokenization processing step which can be read within the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.04805\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"BERT paper\"), \" and tensorflow's \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"text.BertTokenizer\")), \" function.\"))), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/34013f2ee57b306d2b4fc605f2243982/e4b8a/tokenization.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"60.9375%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB7y6pQkS//8QAGBABAQADAAAAAAAAAAAAAAAAAAEQESH/2gAIAQEAAQUCTuNKj//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AUf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhEGH/2gAIAQEAAT8hbGJ1eS46g9eP/9oADAMBAAIAAwAAABBUH//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/EIP/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPxBX/8QAHRABAQABBAMAAAAAAAAAAAAAAREAECExQVFxgf/aAAgBAQABPxBgRY9Bcm0PCbP3QCmvq4JwHBs4kEtj3n//2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Tokenization\",\n    \"title\": \"Tokenization\",\n    \"src\": \"/static/34013f2ee57b306d2b4fc605f2243982/212bf/tokenization.jpg\",\n    \"srcSet\": [\"/static/34013f2ee57b306d2b4fc605f2243982/7809d/tokenization.jpg 192w\", \"/static/34013f2ee57b306d2b4fc605f2243982/4ecad/tokenization.jpg 384w\", \"/static/34013f2ee57b306d2b4fc605f2243982/212bf/tokenization.jpg 768w\", \"/static/34013f2ee57b306d2b4fc605f2243982/5ef17/tokenization.jpg 1152w\", \"/static/34013f2ee57b306d2b4fc605f2243982/e4b8a/tokenization.jpg 1207w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Tokenization\"), \"\\n  \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Tagging\"), \": typically follows Tokenization in a NLP pipeline. This is the process of tagging each word with a speech tag. Outputs are in the form of \", mdx(\"code\", {\n    parentName: \"li\",\n    \"className\": \"language-text\"\n  }, \"(word, tag)\"), \". For example, \\u201Cnew\\u201D has a tag \\u201CADJ\\u201D (adjective), \\u201Cearly\\u201D has a tag \\u201CADV\\u201D (adverb) and \\u201CEurope\\u201D will have a tag \\u201CNOUN\\u201D (noun). See more tags \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.nltk.org/book/ch05.html#tab-universal-tagset\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"here\"), \".\")), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/00292b3d21976620d6a6173dbecbefec/280b9/types-of-tagging.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"79.16666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMCBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHdrugLwE//xAAaEAACAwEBAAAAAAAAAAAAAAAAAREhMQIS/9oACAEBAAEFAm7mFp0r8yYf/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQMBAT8BJ//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABkQAAIDAQAAAAAAAAAAAAAAAAAQASFBMf/aAAgBAQAGPwIqd1UcX//EABoQAQEBAAMBAAAAAAAAAAAAAAERACFBYXH/2gAIAQEAAT8hhFPmXxfZV1hAI0cqEHrgBAy80wAAIG//2gAMAwEAAgADAAAAEAwP/8QAFhEAAwAAAAAAAAAAAAAAAAAAARAR/9oACAEDAQE/ECi//8QAFxEBAAMAAAAAAAAAAAAAAAAAARARUf/aAAgBAgEBPxBo2P/EABwQAQADAAMBAQAAAAAAAAAAAAEAESExQVFhcf/aAAgBAQABPxAQJJquaizJbHHj9hlgWJ3Nrdp+Q+TTwQZJ7DLAoDqf/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Types of Tags\",\n    \"title\": \"Types of Tags\",\n    \"src\": \"/static/00292b3d21976620d6a6173dbecbefec/212bf/types-of-tagging.jpg\",\n    \"srcSet\": [\"/static/00292b3d21976620d6a6173dbecbefec/7809d/types-of-tagging.jpg 192w\", \"/static/00292b3d21976620d6a6173dbecbefec/4ecad/types-of-tagging.jpg 384w\", \"/static/00292b3d21976620d6a6173dbecbefec/212bf/types-of-tagging.jpg 768w\", \"/static/00292b3d21976620d6a6173dbecbefec/280b9/types-of-tagging.jpg 905w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Types of Tags\"), \"\\n  \"), mdx(\"p\", null, \"Tagging can often be incorrect based on the context, so performance is evaluated based on the gold standard test data. The gold standard test data is the \\u201Ccorrect\\u201D tag as classified by a human expert \\u2013 which could be incorrect and have biases. Nevertheless, the gold standard is widely considered the evaluation metric for automatic tagging.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stemming\"), \": removes the last few letters of words that can be defined the same. For example, \\u201Ctravel\\u201D, \\u201Ctravelling\\u201D and travelled\\u201D are all verbs describing the same action. There are various types of stemmers: Lancaster, Snowball and Porter. Read \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stackoverflow.com/a/11210358\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"this\"), \" stackoverflow post.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lemmatization\"), \": using knowledge base called WordNet, you can convert similar words into the root word. For example, \\u201Cate\\u201D and \\u201Ceat\\u201D.\"))), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"399px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/cd7f4bafaa42639deb999b1580bea69f/a307d/stemming-vs-lemmatization.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.770833333333332%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsTAAALEwEAmpwYAAABR0lEQVQY032PTUvCAByH9yX9ECFRR6GrpyC9KFRUF8MoQbFt5ghzaKhzM1P3qtvc2ktjrDHxLbTtH9q95/p7Dr8HAQBtOkUxjKjVXkiyUChUKhWWZWGPJEkURT3iONlsoij6hFe7HZqoEqViSRAEZL1ea4bB0zSTTAoYZjhOu9WiutRi6wPAYj7/ms3G+fw4l1M/DLxzm6ufEvS9ZVlBECCmab4PBtxwKKVSK45bbjY8x7IjjtMYAIjCEADMTEY+Pgp1c2hT2fohOjjfTRAhrusKkiQxDJ9Of7uubpnKRHntPbMqvTOiaNnvS4nEWywW1OrO0jtDD67IE95qR1GE/Gy3sqbxjcYoHl+pqr+YT0SZoB8cX//LNotFNZtVLi/8seKtXIy5QXvXRO8uDEMEAD5tm5dlvVzeeJ7r+6IoqrboeV64//wPv+i2LmSu0RpYAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Stemming and Lemmatization\",\n    \"title\": \"Stemming and Lemmatization\",\n    \"src\": \"/static/cd7f4bafaa42639deb999b1580bea69f/a307d/stemming-vs-lemmatization.png\",\n    \"srcSet\": [\"/static/cd7f4bafaa42639deb999b1580bea69f/8514f/stemming-vs-lemmatization.png 192w\", \"/static/cd7f4bafaa42639deb999b1580bea69f/804b2/stemming-vs-lemmatization.png 384w\", \"/static/cd7f4bafaa42639deb999b1580bea69f/a307d/stemming-vs-lemmatization.png 399w\"],\n    \"sizes\": \"(max-width: 399px) 100vw, 399px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Stemming and Lemmatization\"), \"\\n  \"), mdx(\"p\", null, \"The general purpose behind \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stemming\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lemmatization\"), \" are to get words with the same meaning to be modelled together. \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Vectorization\"), \": converts text into numbers. There are many forms of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"vectorization\"), \", one popular way is the bag of words technique. For example, the sentences \\u201Cthis is great.\\u201D and \\u201Cbut this is not great\\u201D would become \", mdx(\"code\", {\n    parentName: \"li\",\n    \"className\": \"language-text\"\n  }, \"sentence_1 = [1, 1, 1, 0, 0]\"), \" and \", mdx(\"code\", {\n    parentName: \"li\",\n    \"className\": \"language-text\"\n  }, \"sentence_2 = [1, 1, 1, 1, 1]\"), \". Because \", \"[\\u201Cthis\\u201D, \\u201Cis\\u201D, \\u201Cgreat\\u201D, \\u201Cbut\\u201D, \\u201Cnot\\u201D]\", \" is the order of the columns. \")), mdx(\"p\", null, \"Another popular way is count vectorization where we replace the word with the number of occurances within the corpus. However, this technique has mixed results.\"), mdx(\"h2\", null, \"Implementing NLP\"), mdx(\"p\", null, \"Although NLP can seem like there are many possible solutions it is fairly easy to implement. There are many open source tools that allows us to implement state of art NLP models and preprocessing steps. We can use the Natural Language Toolkit \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.nltk.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"NLTK\"), \" package for NLP tasks, \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tensorflow.org/hub\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"TensorFlow Hub\"), \" for transfer learning models and, more recently, the very popular \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://huggingface.co/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"HuggingFace\"), \" library. Lets talk about the Multi-head attention layer which is widely consdiered a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://paperswithcode.com/method/multi-head-attention\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"big breakthrough\"), \" in NLP - gone are the times of vanilla RNNs and LSTMs! \"), mdx(\"h2\", null, \"Multi-Head Attention\"), mdx(\"p\", null, \"Multi-head Attention layer is based on the Scaled Dot Product Attention layer. In short you can think of it as a differentiable dictionary lookup that attempts to learn whether a word is a verb, subject, noun etc. but remember that each word is represented as a vector (learned during training).\"), mdx(\"p\", null, \"The encoder within the layer represents the words as vectors. Suppose the decoder already has translated the beginning of a sentence, it will now look for the next logical word to translate. For example, \\\"They played tennis\\\", the decoder translates the subject \\\"They\\\" and now looks for the verb based on its vectorized representation, called a query. The query and the key to the word (i.e. the encoder's representation) are compared and similiarity measure is used.\"), mdx(\"p\", null, \"Having multiple of these types of layers allows the model to represent a word into different subspaces. All information about the word is split across the layers such as whether the word is a verb, past tense, etc. Machine translation is extremely broad and can definetly not be summarised within a sub section of a blog post. I would recommend \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://web.stanford.edu/class/cs224n/s\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"this\"), \" Stanford course for an in-depth appreciation on the extensive work being carried out within the field of machine transation. \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://paperswithcode.com/search?q=author%3AAshish+Vaswani\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Ashish Vaswani\"), \" was the lead researcher on creating the attention is all you need (nice name) architecture. \"), mdx(\"p\", null, \"Tensorflow has many Transformer based architectures that have been successful in performing many NLP tasks. This is a promising architecture for NLP tasks and many state of the art models contain the attention mechanism architecture in part. \"), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"As mentioned, you should not be building the architecture of your NLP model. There are many models that can be customised for your use case on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://tfhub.dev/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"TensorFlow Hub\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://huggingface.co/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"HuggingFace\"), \". These libaries even include functions that can be used to preprocess your data to the model you have selected.\"), mdx(\"h4\", null, \"Further reading:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Chapter 1 of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.nltk.org/book/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Natural Language Processing with Python\"), \" book. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Stanford's \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.stanford.edu/class/cs224n/s\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"CS224N Course\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Attention is all you need \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.03762\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"paper\"))));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":5,"excerpt":"Natural Language Processing (NLP) is a bit of a buzzword nowadays with job posts asking for 10 years of experience on the BERT model that…","frontmatter":{"title":"Natural Language Processing","description":"Natural Language Processing is an extensive topic and this blog tries to summarise it.","cover":{"publicURL":"/static/23f3810b769821db9ccc2f180c4a95bb/cover_5.jpeg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAMEBf/EABYBAQEBAAAAAAAAAAAAAAAAAAABA//aAAwDAQACEAMQAAAB2aAzkFf/xAAYEAACAwAAAAAAAAAAAAAAAAAAAQISMf/aAAgBAQABBQJE7IWf/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAD/9oACAEDAQE/AdyC/8QAFREBAQAAAAAAAAAAAAAAAAAAAhD/2gAIAQIBAT8BE//EABkQAAEFAAAAAAAAAAAAAAAAAAEAAhAhYf/aAAgBAQAGPwKLYTq//8QAFxABAQEBAAAAAAAAAAAAAAAAEQABIf/aAAgBAQABPyGdaCct6b3/2gAMAwEAAgADAAAAEPQv/8QAGREAAgMBAAAAAAAAAAAAAAAAAAERITGx/9oACAEDAQE/EKJTwkSvh//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPxCmv//EABoQAQACAwEAAAAAAAAAAAAAAAEAESFBYVH/2gAIAQEAAT8QSiQo09D7E2DWBhTvZ//Z"},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/23f3810b769821db9ccc2f180c4a95bb/ee7c2/cover_5.jpg","srcSet":"/static/23f3810b769821db9ccc2f180c4a95bb/74341/cover_5.jpg 625w,\n/static/23f3810b769821db9ccc2f180c4a95bb/c98fe/cover_5.jpg 1250w,\n/static/23f3810b769821db9ccc2f180c4a95bb/ee7c2/cover_5.jpg 2500w","sizes":"(min-width: 2500px) 2500px, 100vw"},"sources":[{"srcSet":"/static/23f3810b769821db9ccc2f180c4a95bb/44552/cover_5.avif 625w,\n/static/23f3810b769821db9ccc2f180c4a95bb/fdd2c/cover_5.avif 1250w,\n/static/23f3810b769821db9ccc2f180c4a95bb/d8633/cover_5.avif 2500w","type":"image/avif","sizes":"(min-width: 2500px) 2500px, 100vw"},{"srcSet":"/static/23f3810b769821db9ccc2f180c4a95bb/a07b5/cover_5.webp 625w,\n/static/23f3810b769821db9ccc2f180c4a95bb/f1147/cover_5.webp 1250w,\n/static/23f3810b769821db9ccc2f180c4a95bb/0676d/cover_5.webp 2500w","type":"image/webp","sizes":"(min-width: 2500px) 2500px, 100vw"}]},"width":2500,"height":1000}}},"coverAlt":"This Image","datePublished":"2021-11-09","dateModified":"2021-11-09","category":"NLP","tags":["NLP","TensorFlow","HuggingFace"]},"fields":{"slug":"/natural-language-processing","route":"/natural-language-processing","pathName":"/natural-language-processing","url":"https://ashishthanki.github.io/natural-language-processing"},"internal":{"content":"---\ntitle: \"Natural Language Processing\"\ncover: images/cover_5.jpeg\ncoverAlt: \"This Image\"\ndescription: \"Natural Language Processing is an extensive topic and this blog tries to summarise it.\"\ndatePublished: \"2021-11-09\"\ndateModified: \"2021-11-09\"\ncategory: \"NLP\"\ntags:\n  - NLP\n  - TensorFlow\n  - HuggingFace\n---\n\n<!-- markdownlint-disable-->\n\nNatural Language Processing (NLP) is a bit of a buzzword nowadays with job posts asking for 10 years of experience on the BERT model that has only been around since 2018. NLP is still an active area of research and is changing very quickly every year. With new architectures and layers being wrapped together for the finest of improvements. This blog gives an overview of NLP and provides resources to allow you to explore further.\n\n\n## Natural Language Processing\n\nNatural language processing is the interaction between computers and human languages. Text data ultimately needs to be processed in such a way that computers can understand and analyse in order to gain hidden insight about the data. \n\nNLP unlocks the door for solving many problems, such as, automatic summarization, translation, sentiment analysis, speech recognition and topic segmentation. Text data is all around us, in reports, books, comments, blog posts, news articles, emails and more. Having to look at this data manually can be troublesome. \n\nA good example of using NLP is interpreting surveys. Imagine, you carry out a survey asking individuals from various countries how they feel about their national government. Going through individual descriptions would take a long time. Instead, using NLP we can classify each comment into negative, neutral or positive. Grouping the comments into countries so we can then perform data aggregation. Giving a clearer picture on how individuals feel about their government. This is just the tip of what NLP can offer and is called Sentiment Analysis, probably one of the widely known applications.\n\nWithout taking you back to primary/elementary school, we know that words can be “classed” under nouns, verbs, adjectives and adverbs. These classes are known as lexical categories and form the foundation for many NLP tasks so read up on them (here)[https://www.bbc.co.uk/bitesize/guides/zx3gr82/revision/5] and come back! If you have a background in tabular data, you can think of these lexical categories as features. The more useful the feature that we can extract about the data the more useful the NLP model will be at its task.\n\n\n## Cleaning text data types\n\nIn my opinion, the most challenging part about working with text data is the data cleaning and processing stage. Most models will be off-the-shelf from TensorFlow Hub. It is unlikely that you'll be training your own NLP model. It is, however, likely that you'll have to preprocess your input data so that it is in the correct format (and shape!) for the selected model. Rather than providing the inidividuals steps for a single model, below are a few popular data processing techniques. Remember, the aim of these techniques are to provide additional features that the model can learn from.\n\n![Text Cleaning Pipeline](./images/natural-language-processing/text-pipeline.png)\n\n- **Removing StopWords**: words that add no value to the interpretation of the data should be removed. For example, words like “an”, “the”, “a”, “be” etc. create noise while modelling. \n\n- **Tokenization**: break down the unstructured text data into chunks of information so it can be counted as discrete elements. For example, split a sentence into individual words. The BERT model has a specific tokenization processing step which can be read within the [BERT paper](https://arxiv.org/abs/1810.04805) and tensorflow's [`text.BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer) function.\n\n![Tokenization](./images/natural-language-processing/tokenization.jpeg)\n\n- **Tagging**: typically follows Tokenization in a NLP pipeline. This is the process of tagging each word with a speech tag. Outputs are in the form of `(word, tag)`. For example, “new” has a tag “ADJ” (adjective), “early” has a tag “ADV” (adverb) and “Europe” will have a tag “NOUN” (noun). See more tags [here](http://www.nltk.org/book/ch05.html#tab-universal-tagset).\n\n\n![Types of Tags](./images/natural-language-processing/types-of-tagging.jpeg)\n\nTagging can often be incorrect based on the context, so performance is evaluated based on the gold standard test data. The gold standard test data is the “correct” tag as classified by a human expert – which could be incorrect and have biases. Nevertheless, the gold standard is widely considered the evaluation metric for automatic tagging.\n\n\n- **Stemming**: removes the last few letters of words that can be defined the same. For example, “travel”, “travelling” and travelled” are all verbs describing the same action. There are various types of stemmers: Lancaster, Snowball and Porter. Read [this](https://stackoverflow.com/a/11210358) stackoverflow post.\n\n- **Lemmatization**: using knowledge base called WordNet, you can convert similar words into the root word. For example, “ate” and “eat”.\n\n![Stemming and Lemmatization](./images/natural-language-processing/stemming-vs-lemmatization.png)\n\nThe general purpose behind **Stemming** and **Lemmatization** are to get words with the same meaning to be modelled together. \n\n- **Vectorization**: converts text into numbers. There are many forms of [vectorization](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html), one popular way is the bag of words technique. For example, the sentences “this is great.” and “but this is not great” would become `sentence_1 = [1, 1, 1, 0, 0]` and `sentence_2 = [1, 1, 1, 1, 1]`. Because [“this”, “is”, “great”, “but”, “not”] is the order of the columns. \n\nAnother popular way is count vectorization where we replace the word with the number of occurances within the corpus. However, this technique has mixed results.\n\n\n## Implementing NLP \n\nAlthough NLP can seem like there are many possible solutions it is fairly easy to implement. There are many open source tools that allows us to implement state of art NLP models and preprocessing steps. We can use the Natural Language Toolkit [NLTK](https://www.nltk.org/) package for NLP tasks, [TensorFlow Hub](https://www.tensorflow.org/hub) for transfer learning models and, more recently, the very popular [HuggingFace](https://huggingface.co/) library. Lets talk about the Multi-head attention layer which is widely consdiered a [big breakthrough](https://paperswithcode.com/method/multi-head-attention) in NLP - gone are the times of vanilla RNNs and LSTMs! \n\n\n## Multi-Head Attention\n\nMulti-head Attention layer is based on the Scaled Dot Product Attention layer. In short you can think of it as a differentiable dictionary lookup that attempts to learn whether a word is a verb, subject, noun etc. but remember that each word is represented as a vector (learned during training).\n\nThe encoder within the layer represents the words as vectors. Suppose the decoder already has translated the beginning of a sentence, it will now look for the next logical word to translate. For example, \"They played tennis\", the decoder translates the subject \"They\" and now looks for the verb based on its vectorized representation, called a query. The query and the key to the word (i.e. the encoder's representation) are compared and similiarity measure is used.\n\nHaving multiple of these types of layers allows the model to represent a word into different subspaces. All information about the word is split across the layers such as whether the word is a verb, past tense, etc. Machine translation is extremely broad and can definetly not be summarised within a sub section of a blog post. I would recommend [this](http://web.stanford.edu/class/cs224n/s) Stanford course for an in-depth appreciation on the extensive work being carried out within the field of machine transation. [Ashish Vaswani](https://paperswithcode.com/search?q=author%3AAshish+Vaswani) was the lead researcher on creating the attention is all you need (nice name) architecture. \n\nTensorflow has many Transformer based architectures that have been successful in performing many NLP tasks. This is a promising architecture for NLP tasks and many state of the art models contain the attention mechanism architecture in part. \n\n## Conclusion\n\nAs mentioned, you should not be building the architecture of your NLP model. There are many models that can be customised for your use case on [TensorFlow Hub](https://tfhub.dev/) and [HuggingFace](https://huggingface.co/). These libaries even include functions that can be used to preprocess your data to the model you have selected.\n\n\n#### Further reading:\n\n- Chapter 1 of [Natural Language Processing with Python](http://www.nltk.org/book/) book. \n- Stanford's [CS224N Course](http://web.stanford.edu/class/cs224n/s)\n- Attention is all you need [paper](https://arxiv.org/abs/1706.03762)\n\n"}}},"pageContext":{"slug":"/natural-language-processing","nexttitle":"Pandas Efficient Programming","nextslug":"/pandas-efficient-programming","prevtitle":"Using Git","prevslug":"/using-git","relatedPosts":[{"title":"Model Drift and Concept Drift","description":"Maintaining a good in-production model requires understanding the understanding of model and concept drift.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd1ORYA//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAQABBQKaOI//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAIDH/2gAIAQEABj8CKv8A/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAERITFRQf/aAAgBAQABPyHNYn6IdFrWX1iQoP/aAAwDAQACAAMAAAAQA8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMXFBof/aAAgBAQABPxBVtSpsKgjtDxmHQ6xbodZwxdvtBMZ7P//Z"},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg","srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/76a0b/degradation-point.jpg 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f5d63/degradation-point.jpg 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/cc266/degradation-point.jpg 1180w","sizes":"(min-width: 590px) 590px, 100vw"},"sources":[{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/8ec5f/degradation-point.avif 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/24e48/degradation-point.avif 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/23579/degradation-point.avif 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/630d8/degradation-point.avif 1180w","type":"image/avif","sizes":"(min-width: 590px) 590px, 100vw"},{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/a168a/degradation-point.webp 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/6e9d7/degradation-point.webp 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7b043/degradation-point.webp 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f3c07/degradation-point.webp 1180w","type":"image/webp","sizes":"(min-width: 590px) 590px, 100vw"}]},"width":590,"height":368},"coverImageAlt":"Deployed models have a degradation point and we need to avoid it!","datePublished":"2023-07-31T00:00:00.000Z","dateModified":"2023-07-31T00:00:00.000Z","category":"Ops","tags":["Machine Learning","DevOps"],"excerpt":"Model drift is a silent killer for in-production models and something that data scientists need to be mindful of. The importance of MLOps…","timeToRead":4,"slug":"/model-drift-and-concept-drift","route":"/model-drift-and-concept-drift","pathName":"/model-drift-and-concept-drift","url":"https://ashishthanki.github.io/model-drift-and-concept-drift"},{"title":"End to End Data Science Workflow","description":"The typical data science E2E workflow and pipeline.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAPoAAAD6AG1e1JrAAADbklEQVQ4y5WS21MadxTH96/qax/iTKcPaaftU6cPtlNNO5NMk3aSmo4dUzQxahHiDUEu4u4ChusKi7C4sMslK7DAEnRZQNAoqUhrq2hbE9ndXwfodKYvNj1z3s58vmfO93ugcrms0WiMRiOGYVar1eFwAABkWQZvUNDFxYVGo9FqtQqFYmRkRK1W/w/49PQ0nU5zHMcwDE3TJEm2Wi0AgCiKkiT9B7yzsxMIBBAECQaDJEnCMMyy7Bsuh+r1+hqG0RRFU5TH7fL5vKEQUSqXtnm+KAiyJF+hAlWqVTOKEiRFkNFVO+Z0rsEr6FPkqR/z4phHFMWrNle3t0jbKuXSB2zqML4UWkdcOKz3ac0+hzUYKXLV5k/HvSt69S+4wbK8zbntHGGNn+X9t2KhO0Zs/BvvV6NunWKNMk2txomOBZLYBv+QstwTg+qlUmN/r76bE7KhMufdF+KlciqcDYTYZLZ2WCvUmy9P2h1KbkvglSi3uxK9GKBCgXfNh2m0GDXzKiT6MF2Z2qw+wVbWE2P24A+28EO9+V7coiDhsah1LLigTBvNofn5hMF4UK1CR80GMhzAhjl8KHP/tue9UPZdd+pt5eQDw7XvtX3DS9fuqt7yTbxjG+rbGO8L3X1//dbXcH9/8PMBXKWCXuy+jFiEnO+Ide1b8Jxxq27KNVAS90cX1xMGX1yP4YsZfDnnNyUcc2GTNotaUgjCmM3NgwPoRS1P4I88zlHcOxELzjJ+ZYZ4vLNFdn256sM6hvHPaffygN1w0274Qj320bTi+uyj65hz4fi3k929am231mqdACBJ4iWQRSBLQOq0LIkdWJaB1PVPAuDZxkLA8rHTNLAG9yNzH6wufpjAPilwse5YEiXQ60uxg8gygLq5/51+87BSKpB8PsJzRCrhz2fCxecJB/IjQ6FMBElGYYYyx0lzkkZSMZjecECSJJ+dvz5t/XnYOG4c/dr85ezn4/Pf/2hfivLFK+ns/LV+5ubK/KBuenB57kud6sbM+Kd69Q29elA3Owzt75WYgKLKLZXTqlJyUticqqRVXusQqrvjQe97rd/B2tte67f85nQhoeSiE2x4NEc9Tm88iEfsUOPwgCEm+c15PmUSMhaBRUsZ+BkxS7gnY8EnlE9J41NJckZgTcWUXkgby1mkkoPz8Tk+H/0LpmKVQKg7XHsAAAAASUVORK5CYII="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c4dadd7c4934e3b8fb089c0df362eade/1198f/data-science-process.webp","srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/4be21/data-science-process.webp 97w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/2061a/data-science-process.webp 194w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/1198f/data-science-process.webp 388w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/cfa69/data-science-process.webp 776w","sizes":"(min-width: 388px) 388px, 100vw"},"sources":[{"srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/6ee32/data-science-process.avif 97w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/bfa01/data-science-process.avif 194w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/4a107/data-science-process.avif 388w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/1ee87/data-science-process.avif 776w","type":"image/avif","sizes":"(min-width: 388px) 388px, 100vw"}]},"width":388,"height":368},"coverImageAlt":"E2E Pipeline","datePublished":"2023-07-15T00:00:00.000Z","dateModified":"2023-07-15T00:00:00.000Z","category":"Data Science","tags":["Machine Learning"],"excerpt":"The data science workflow can be cumbersome for beginners to handle and often some parts are forgotten completely, such as splitting the…","timeToRead":7,"slug":"/end-to-end-data-science-workflow","route":"/end-to-end-data-science-workflow","pathName":"end-to-end-data-science-workflow","url":"https://ashishthanki.github.io/end-to-end-data-science-workflow"}]}},
    "staticQueryHashes": ["3661114550"]}