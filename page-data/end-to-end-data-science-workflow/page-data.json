{
    "componentChunkName": "component---themes-advanced-src-templates-post-query-ts",
    "path": "/end-to-end-data-science-workflow",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"End to End Data Science Workflow\",\n  \"cover\": \"images/data-science/data-science-process.png\",\n  \"coverAlt\": \"E2E Pipeline\",\n  \"description\": \"The typical data science E2E workflow and pipeline.\",\n  \"datePublished\": \"2023-07-15\",\n  \"dateModified\": \"2023-07-15\",\n  \"category\": \"Data Science\",\n  \"tags\": [\"Machine Learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"The data science workflow can be cumbersome for beginners to handle and often some parts are forgotten completely, such as splitting the data set into test, validation and training data. In this blog post I cover each step of a typical data science workflow and explain what we will achieve during the step, some of my watch-its and the packages or tools that can help you along the way - know what size your data is.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Apart from the data science workflow below, I would recommend adopting a software development approach by adding version control (git), package and dependency management (poetry or conda), testing frameworks (pytest) and following the SOLID principles if you are using Python. All of which I have covered in previous blog posts!\")), mdx(\"h2\", null, \"1. Data Ingestion\"), mdx(\"p\", null, \"The first step is to understand where your data is coming from and how to get this data into the required format, structure and quality. This can often take a very long time and may end up extremely time consuming if your data is scattered across many different sources.\"), mdx(\"h2\", null, \"2. Data storage\"), mdx(\"p\", null, \"The data storage may require a bit of help from more experienced colleagues or a software development team. The solution could be as simple as storing the data locally or as 'complex' as multiple AWS Relational database instances or S3 buckets. Either way, storing and accessing the data is the first step. \"), mdx(\"h2\", null, \"3. Importing Data\"), mdx(\"p\", null, \"If there is a large amount of data (more than x10 billion) and it cannot be stored in-memory then pandas might not work for you, although, one could argue that you could make it work by using the technique described on my Efficient Pandas blog.\"), mdx(\"p\", null, \"Another approach would be to use \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"cuda\"), \" or chunking the data using \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"numpy\"), \" or \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"pandas\"), \".\"), mdx(\"p\", null, \"PySpark may be an option because it uses parallel operations in a cluster - when this blog was written I have never \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"needed\"), \" to use this solution. Instead, I worked on the principle of 'load only what needs to be loaded', if you don't need the ID column or all numerical columns then don't load or save it in the file in the first place! I would also encourage changing the data types of each column when you do manage to load it into a dataframe.\"), mdx(\"p\", null, \"Either way loading the data could be a big challenge and may require hardware upgrades or even using cloud computing tools such as AWS Data Brew Glue and Amazon SageMaker Data Wrangler which are both written in PySpark - so technically I have used PySpark.\"), mdx(\"p\", null, \"Before we move on to the 'fun' part, after you have loaded your data into memory (or other way) make sure you split your data into test, validation and training. No peaking in the test dataset. We want to avoid data leakage, aka cheating!\"), mdx(\"h2\", null, \"4. Data exploration and data processing\"), mdx(\"p\", null, \"This step involves exploring the data, but you also need to be mindful of the next step of data cleaning. This is probably the bulk of the project and a key skill you need in data science. Both these steps will be performed hand in hand and several loops will be made between them before we get to modelling at all.\"), mdx(\"p\", null, \"You can use python based tools like \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://matplotlib.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Matplotlib\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://seaborn.pydata.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Seaborn\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hvplot.holoviz.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"hvPlot\"), \" or even dashboarding tools like \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://learn.microsoft.com/en-us/power-bi/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Power BI\"), \".\"), mdx(\"h2\", null, \"5. Data cleaning and preparing for ML\"), mdx(\"p\", null, \"As mentioned, this step goes hand in hand with data exploration and processing. We need to clean up the misspelled words, remove obvious outliers and anomalies, impute missing values, combing features or remove features entirely (more on that later!), encoding the data (e.g. one hot encoding, ordinal encoding), or scaling. The list honestly goes on, you may even figure that you ingested data incorrectly from source and would have to start all over again!\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"You don't need to scale (i.e. MinMax or Standard Scale) your data if you plan on using a tree based method, but you will if you are using a gradient based such as generalized linear models. This is because tree based do not require scaling. \")), mdx(\"p\", null, \"More preprocessing steps can be found on Sci-kit Learn \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://scikit-learn.org/stable/modules/preprocessing.html#\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"here\"), \".\"), mdx(\"h2\", null, \"6. Ditch the jupyter notebook and make \", mdx(\"code\", {\n    parentName: \"h2\",\n    \"className\": \"language-text\"\n  }, \".py\"), \" files\"), mdx(\"p\", null, \"At this point you may have been using the jupyter notebook or interactive VS code window. Ditch it and start coding in \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \".py\"), \" files.\"), mdx(\"p\", null, \"This encourages good software development and allows you to change the pipeline that you hopefully should have created in step 4. This makes your code more repeatable and the mess that jupyter notebook creates when you start changing the cell locations - we have all done!\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"I was once given a jupter notebook with specific instructions on which cells to run. Never do that. Your notebook should run from top to bottom, and I encourage splitting the notebooks up so that each step in the data science process has its own jupyter notebook, with the exception of data exploration step which can be 2 or more! \")), mdx(\"p\", null, \"At this point, you might want to add version control and testing to the project so we can track changes and minimise code or data changes introducing bugs in production.\"), mdx(\"p\", null, \"Also, while you are at it set the seed to the project at the top of your main function or wherever you feel randomization might affect things, you don't want randomness to ruin the repeatability of your 'good' model performance. I also would set the \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"ramdom_state\"), \" of your model too.\"), mdx(\"h2\", null, \"7. Modelling\"), mdx(\"p\", null, \"Stop. Don't go all guns blazing and use the most complex model you have heard of. Start simple and work your way up, being aware of the metrics that you should train your model on. \"), mdx(\"p\", null, \"The popular metrics and, also the ones I have used are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Regression Supervised Model: mean squared error, mean absolute error, room mean squared error, max error, r2 and adjusted r2*.\")), mdx(\"p\", null, \"*Send me a message on LinkedIn if you need to learn more about this one! ;).\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Classification Supervised Model: accuracy, f1, precision, recall and AUC (technical not a metric but a good graph to plot).\")), mdx(\"p\", null, \"There are many more metrics that you can check out on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"this\"), \" Sci-Kit Learn webpage, be sure to understand the one you are being asked or have chosen to use! Also, don't be afraid to make your own metric(s) and use the domain experts that are within your company - data scientists need to be holistic in their approach to solving problems. \"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Side note, if you see an \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"r2\"), \" of 1, or, \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"RMSE\"), \" score of 0, or, an \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"accuracy\"), \" of 0.99. Don't get excited, it is likely there was some sort of data leakage and your model is over-fitting. Fight the temptation of using the test set to evaluate the model. Instead, continue reading below!\")), mdx(\"h2\", null, \"8. Evaluate Model Performance\"), mdx(\"p\", null, \"Since we split the dataset into test, validation and training, you use the validation set to evaluate the viability of the chosen model and then compare one or several metrics to understand if the model is doing what you think it is doing. The validation set can also be used to compare your models.\"), mdx(\"h2\", null, \"9. Tuning\"), mdx(\"p\", null, \"Tuning can be fun and you can use several frameworks that have asynchronous or parallel backends -  \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://optuna.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"optuna\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.ray.io/en/latest/tune/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ray\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://hyperopt.github.io/hyperopt/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"hyperopt\"), \" to name a few. You could go brute force and do \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Grid Search\"), \" or \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Randomized Search\"), \" too but I would avoid those since when we deploy the model into production this would take a lot of unnecessary compute and therefore, monies \\xA3\\xA3\\xA3\\xA3!!!\"), mdx(\"p\", null, \"Instead, opt in for a 'bayesian' tuning approach that efficiently searches and prunes trials. Meaning, it uses prior information based on previous hyperparameter runs while tracking model performance to change the parameters used next. It dynamically construct the search space and the parameters we use next. For example, if performance drops when decreasing learning rate, say \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"0.015\"), \", tuning framework would try run larger learning rates, say \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"0.02\"), \", eventually making its way to the optimal value, say \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"0.05\"), \". If model performance increases then it would continue doing a deeper dive!\"), mdx(\"h2\", null, \"10. Feature importance\"), mdx(\"p\", null, \"Now that you have the best model and parameters, we now need to understand how good the features are. Again, there are many ways to do this, which I have covered in my Interoperability \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"./category/Interpretability\"\n  }, \"blogs\"), \".\"), mdx(\"p\", null, \"You have simple ways such as the model gradients for a linear model or feature importance values defined as attributes within the model object, or the many ways you can obtain feature importance described in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"./category/Interpretability\"\n  }, \"blogs\"), \".\"), mdx(\"p\", null, \"Once you do this, you may find some features are absolute rubbish. If that is the case then try remove these and run the pipeline and tuning steps again.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"WARNING! Do not remove features if it is a requirement for your domain to understand how bad the features are. If the domain requires stock data to be included you must include it. If you are doing a Kaggle/practice dataset, do whatever you want.\")), mdx(\"p\", null, \"Lastly, plot all the feature importances and just from logic does it make sense? Do the stand out features align with your EDA and business logic?\"), mdx(\"p\", null, \"This may be the last step for you as some domains require interoperability to use part of a wider analysis or decision making task and may not need a traditional prediction given input data per say. However, if your model is going to be used for prediction then carry on reading.\"), mdx(\"h2\", null, \"11. Test set\"), mdx(\"p\", null, \"Finally, you made it this far, which means you understand your features, data and model very well. From hereon, there are no more model changes because we now need to evaluate model performance against the hidden away test dataset.\"), mdx(\"p\", null, \"The metrics that you evaluate using the test dataset will be what you will observe in production and should be the reported figure you give back the chain of command.\"), mdx(\"h2\", null, \"12. MLOps and Model drift\"), mdx(\"p\", null, \"Once you evaluate the performance on the test dataset, you should now add to the pipeline to trigger hyperparameter tuning when model performance drops below a certain threshold. Read more about it in my MLOperation \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"./ml-ops\"\n  }, \"blog\"), \". \"), mdx(\"h1\", null, \"13. Plotting and Summarise\"), mdx(\"p\", null, \"Make more nice plots which explains your model and its performance against the test set and whether it is under-fitting or over-fitting the dataset or even better perhaps explain the precision-recall tradeoff. The idea here is to summarise your model performance and make sure others understand it through visuals and brief non-technical sentences.\"), mdx(\"h1\", null, \"Conclusion\"), mdx(\"p\", null, \"Congratulations! You have made it to the end of a typical E2E data science pipeline and workflow. A few iterations of this typical workflow will you give you much more confidence and prepares you to adapt to the domain changes - as domain changes this workflow changes too so watch out! Regardless, the best thing is to start practising now! \"));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":7,"excerpt":"The data science workflow can be cumbersome for beginners to handle and often some parts are forgotten completely, such as splitting the…","frontmatter":{"title":"End to End Data Science Workflow","description":"The typical data science E2E workflow and pipeline.","cover":{"publicURL":"/static/c4dadd7c4934e3b8fb089c0df362eade/data-science-process.png","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAPoAAAD6AG1e1JrAAADbklEQVQ4y5WS21MadxTH96/qax/iTKcPaaftU6cPtlNNO5NMk3aSmo4dUzQxahHiDUEu4u4ChusKi7C4sMslK7DAEnRZQNAoqUhrq2hbE9ndXwfodKYvNj1z3s58vmfO93ugcrms0WiMRiOGYVar1eFwAABkWQZvUNDFxYVGo9FqtQqFYmRkRK1W/w/49PQ0nU5zHMcwDE3TJEm2Wi0AgCiKkiT9B7yzsxMIBBAECQaDJEnCMMyy7Bsuh+r1+hqG0RRFU5TH7fL5vKEQUSqXtnm+KAiyJF+hAlWqVTOKEiRFkNFVO+Z0rsEr6FPkqR/z4phHFMWrNle3t0jbKuXSB2zqML4UWkdcOKz3ac0+hzUYKXLV5k/HvSt69S+4wbK8zbntHGGNn+X9t2KhO0Zs/BvvV6NunWKNMk2txomOBZLYBv+QstwTg+qlUmN/r76bE7KhMufdF+KlciqcDYTYZLZ2WCvUmy9P2h1KbkvglSi3uxK9GKBCgXfNh2m0GDXzKiT6MF2Z2qw+wVbWE2P24A+28EO9+V7coiDhsah1LLigTBvNofn5hMF4UK1CR80GMhzAhjl8KHP/tue9UPZdd+pt5eQDw7XvtX3DS9fuqt7yTbxjG+rbGO8L3X1//dbXcH9/8PMBXKWCXuy+jFiEnO+Ide1b8Jxxq27KNVAS90cX1xMGX1yP4YsZfDnnNyUcc2GTNotaUgjCmM3NgwPoRS1P4I88zlHcOxELzjJ+ZYZ4vLNFdn256sM6hvHPaffygN1w0274Qj320bTi+uyj65hz4fi3k929am231mqdACBJ4iWQRSBLQOq0LIkdWJaB1PVPAuDZxkLA8rHTNLAG9yNzH6wufpjAPilwse5YEiXQ60uxg8gygLq5/51+87BSKpB8PsJzRCrhz2fCxecJB/IjQ6FMBElGYYYyx0lzkkZSMZjecECSJJ+dvz5t/XnYOG4c/dr85ezn4/Pf/2hfivLFK+ns/LV+5ubK/KBuenB57kud6sbM+Kd69Q29elA3Owzt75WYgKLKLZXTqlJyUticqqRVXusQqrvjQe97rd/B2tte67f85nQhoeSiE2x4NEc9Tm88iEfsUOPwgCEm+c15PmUSMhaBRUsZ+BkxS7gnY8EnlE9J41NJckZgTcWUXkgby1mkkoPz8Tk+H/0LpmKVQKg7XHsAAAAASUVORK5CYII="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c4dadd7c4934e3b8fb089c0df362eade/4b228/data-science-process.webp","srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/0de26/data-science-process.webp 545w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/17d4f/data-science-process.webp 1090w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/4b228/data-science-process.webp 2180w","sizes":"(min-width: 2180px) 2180px, 100vw"},"sources":[{"srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/e0d69/data-science-process.avif 545w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/74715/data-science-process.avif 1090w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/a1b2d/data-science-process.avif 2180w","type":"image/avif","sizes":"(min-width: 2180px) 2180px, 100vw"}]},"width":2180,"height":2065}}},"coverAlt":"E2E Pipeline","datePublished":"2023-07-15","dateModified":"2023-07-15","category":"Data Science","tags":["Machine Learning"]},"fields":{"slug":"/end-to-end-data-science-workflow","route":"/end-to-end-data-science-workflow","pathName":"end-to-end-data-science-workflow","url":"https://ashishthanki.github.io/end-to-end-data-science-workflow"},"internal":{"content":"---\ntitle: \"End to End Data Science Workflow\"\ncover: images/data-science/data-science-process.png\ncoverAlt: \"E2E Pipeline\"\ndescription: \"The typical data science E2E workflow and pipeline.\"\ndatePublished: \"2023-07-15\"\ndateModified: \"2023-07-15\"\ncategory: \"Data Science\"\ntags:\n  - Machine Learning\n---\n\nThe data science workflow can be cumbersome for beginners to handle and often some parts are forgotten completely, such as splitting the data set into test, validation and training data. In this blog post I cover each step of a typical data science workflow and explain what we will achieve during the step, some of my watch-its and the packages or tools that can help you along the way - know what size your data is.\n\n> Apart from the data science workflow below, I would recommend adopting a software development approach by adding version control (git), package and dependency management (poetry or conda), testing frameworks (pytest) and following the SOLID principles if you are using Python. All of which I have covered in previous blog posts!\n\n## 1. Data Ingestion \n\nThe first step is to understand where your data is coming from and how to get this data into the required format, structure and quality. This can often take a very long time and may end up extremely time consuming if your data is scattered across many different sources.\n\n\n## 2. Data storage\n\nThe data storage may require a bit of help from more experienced colleagues or a software development team. The solution could be as simple as storing the data locally or as 'complex' as multiple AWS Relational database instances or S3 buckets. Either way, storing and accessing the data is the first step. \n\n\n## 3. Importing Data \n\nIf there is a large amount of data (more than x10 billion) and it cannot be stored in-memory then pandas might not work for you, although, one could argue that you could make it work by using the technique described on my Efficient Pandas blog.\n\nAnother approach would be to use `cuda` or chunking the data using `numpy` or `pandas`.\n\nPySpark may be an option because it uses parallel operations in a cluster - when this blog was written I have never *needed* to use this solution. Instead, I worked on the principle of 'load only what needs to be loaded', if you don't need the ID column or all numerical columns then don't load or save it in the file in the first place! I would also encourage changing the data types of each column when you do manage to load it into a dataframe.\n\nEither way loading the data could be a big challenge and may require hardware upgrades or even using cloud computing tools such as AWS Data Brew Glue and Amazon SageMaker Data Wrangler which are both written in PySpark - so technically I have used PySpark.\n\nBefore we move on to the 'fun' part, after you have loaded your data into memory (or other way) make sure you split your data into test, validation and training. No peaking in the test dataset. We want to avoid data leakage, aka cheating!\n\n## 4. Data exploration and data processing\n\nThis step involves exploring the data, but you also need to be mindful of the next step of data cleaning. This is probably the bulk of the project and a key skill you need in data science. Both these steps will be performed hand in hand and several loops will be made between them before we get to modelling at all.\n\nYou can use python based tools like [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/), [hvPlot](https://hvplot.holoviz.org/) or even dashboarding tools like [Power BI](https://learn.microsoft.com/en-us/power-bi/).\n\n## 5. Data cleaning and preparing for ML\n\nAs mentioned, this step goes hand in hand with data exploration and processing. We need to clean up the misspelled words, remove obvious outliers and anomalies, impute missing values, combing features or remove features entirely (more on that later!), encoding the data (e.g. one hot encoding, ordinal encoding), or scaling. The list honestly goes on, you may even figure that you ingested data incorrectly from source and would have to start all over again!\n\n> You don't need to scale (i.e. MinMax or Standard Scale) your data if you plan on using a tree based method, but you will if you are using a gradient based such as generalized linear models. This is because tree based do not require scaling. \n\nMore preprocessing steps can be found on Sci-kit Learn [here](https://scikit-learn.org/stable/modules/preprocessing.html#).\n\n## 6. Ditch the jupyter notebook and make `.py` files\n\nAt this point you may have been using the jupyter notebook or interactive VS code window. Ditch it and start coding in `.py` files.\n\nThis encourages good software development and allows you to change the pipeline that you hopefully should have created in step 4. This makes your code more repeatable and the mess that jupyter notebook creates when you start changing the cell locations - we have all done!\n\n> I was once given a jupter notebook with specific instructions on which cells to run. Never do that. Your notebook should run from top to bottom, and I encourage splitting the notebooks up so that each step in the data science process has its own jupyter notebook, with the exception of data exploration step which can be 2 or more! \n\nAt this point, you might want to add version control and testing to the project so we can track changes and minimise code or data changes introducing bugs in production.\n\nAlso, while you are at it set the seed to the project at the top of your main function or wherever you feel randomization might affect things, you don't want randomness to ruin the repeatability of your 'good' model performance. I also would set the `ramdom_state` of your model too.\n\n\n\n## 7. Modelling\n\nStop. Don't go all guns blazing and use the most complex model you have heard of. Start simple and work your way up, being aware of the metrics that you should train your model on. \n\nThe popular metrics and, also the ones I have used are:\n\n- Regression Supervised Model: mean squared error, mean absolute error, room mean squared error, max error, r2 and adjusted r2*.\n\n*Send me a message on LinkedIn if you need to learn more about this one! ;).\n\n- Classification Supervised Model: accuracy, f1, precision, recall and AUC (technical not a metric but a good graph to plot).\n\nThere are many more metrics that you can check out on [this](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) Sci-Kit Learn webpage, be sure to understand the one you are being asked or have chosen to use! Also, don't be afraid to make your own metric(s) and use the domain experts that are within your company - data scientists need to be holistic in their approach to solving problems. \n\n> Side note, if you see an `r2` of 1, or, `RMSE` score of 0, or, an `accuracy` of 0.99. Don't get excited, it is likely there was some sort of data leakage and your model is over-fitting. Fight the temptation of using the test set to evaluate the model. Instead, continue reading below!\n\n## 8. Evaluate Model Performance\n\nSince we split the dataset into test, validation and training, you use the validation set to evaluate the viability of the chosen model and then compare one or several metrics to understand if the model is doing what you think it is doing. The validation set can also be used to compare your models.\n\n## 9. Tuning\n\nTuning can be fun and you can use several frameworks that have asynchronous or parallel backends -  [optuna](https://optuna.org/), [ray](https://docs.ray.io/en/latest/tune/index.html) and [hyperopt](http://hyperopt.github.io/hyperopt/) to name a few. You could go brute force and do [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) or [Randomized Search](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization) too but I would avoid those since when we deploy the model into production this would take a lot of unnecessary compute and therefore, monies ££££!!!\n\nInstead, opt in for a 'bayesian' tuning approach that efficiently searches and prunes trials. Meaning, it uses prior information based on previous hyperparameter runs while tracking model performance to change the parameters used next. It dynamically construct the search space and the parameters we use next. For example, if performance drops when decreasing learning rate, say `0.015`, tuning framework would try run larger learning rates, say `0.02`, eventually making its way to the optimal value, say `0.05`. If model performance increases then it would continue doing a deeper dive!\n\n\n## 10. Feature importance\n\nNow that you have the best model and parameters, we now need to understand how good the features are. Again, there are many ways to do this, which I have covered in my Interoperability [blogs](./category/Interpretability).\n\nYou have simple ways such as the model gradients for a linear model or feature importance values defined as attributes within the model object, or the many ways you can obtain feature importance described in the [blogs](./category/Interpretability).\n\nOnce you do this, you may find some features are absolute rubbish. If that is the case then try remove these and run the pipeline and tuning steps again.\n\n> WARNING! Do not remove features if it is a requirement for your domain to understand how bad the features are. If the domain requires stock data to be included you must include it. If you are doing a Kaggle/practice dataset, do whatever you want.\n\nLastly, plot all the feature importances and just from logic does it make sense? Do the stand out features align with your EDA and business logic?\n\nThis may be the last step for you as some domains require interoperability to use part of a wider analysis or decision making task and may not need a traditional prediction given input data per say. However, if your model is going to be used for prediction then carry on reading.\n\n\n## 11. Test set\n\nFinally, you made it this far, which means you understand your features, data and model very well. From hereon, there are no more model changes because we now need to evaluate model performance against the hidden away test dataset.\n\nThe metrics that you evaluate using the test dataset will be what you will observe in production and should be the reported figure you give back the chain of command.\n\n\n## 12. MLOps and Model drift\n\nOnce you evaluate the performance on the test dataset, you should now add to the pipeline to trigger hyperparameter tuning when model performance drops below a certain threshold. Read more about it in my MLOperation [blog](./ml-ops). \n\n\n# 13. Plotting and Summarise\n\nMake more nice plots which explains your model and its performance against the test set and whether it is under-fitting or over-fitting the dataset or even better perhaps explain the precision-recall tradeoff. The idea here is to summarise your model performance and make sure others understand it through visuals and brief non-technical sentences.\n\n# Conclusion\n\nCongratulations! You have made it to the end of a typical E2E data science pipeline and workflow. A few iterations of this typical workflow will you give you much more confidence and prepares you to adapt to the domain changes - as domain changes this workflow changes too so watch out! Regardless, the best thing is to start practising now! \n"}}},"pageContext":{"slug":"/end-to-end-data-science-workflow","nexttitle":"Python Memory Management and GIL","nextslug":"/python-memory-management-and-gil","prevtitle":"Model Drift and Concept Drift","prevslug":"/model-drift-and-concept-drift","relatedPosts":[{"title":"Model Drift and Concept Drift","description":"Maintaining a good in-production model requires understanding the understanding of model and concept drift.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd1ORYA//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAQABBQKaOI//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAIDH/2gAIAQEABj8CKv8A/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAERITFRQf/aAAgBAQABPyHNYn6IdFrWX1iQoP/aAAwDAQACAAMAAAAQA8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMXFBof/aAAgBAQABPxBVtSpsKgjtDxmHQ6xbodZwxdvtBMZ7P//Z"},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg","srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/76a0b/degradation-point.jpg 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f5d63/degradation-point.jpg 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/cc266/degradation-point.jpg 1180w","sizes":"(min-width: 590px) 590px, 100vw"},"sources":[{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/8ec5f/degradation-point.avif 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/24e48/degradation-point.avif 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/23579/degradation-point.avif 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/630d8/degradation-point.avif 1180w","type":"image/avif","sizes":"(min-width: 590px) 590px, 100vw"},{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/a168a/degradation-point.webp 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/6e9d7/degradation-point.webp 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7b043/degradation-point.webp 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f3c07/degradation-point.webp 1180w","type":"image/webp","sizes":"(min-width: 590px) 590px, 100vw"}]},"width":590,"height":368},"coverImageAlt":"Deployed models have a degradation point and we need to avoid it!","datePublished":"2023-07-31T00:00:00.000Z","dateModified":"2023-07-31T00:00:00.000Z","category":"Ops","tags":["Machine Learning","DevOps"],"excerpt":"Model drift is a silent killer for in-production models and something that data scientists need to be mindful of. The importance of MLOps…","timeToRead":4,"slug":"/model-drift-and-concept-drift","route":"/model-drift-and-concept-drift","pathName":"/model-drift-and-concept-drift","url":"https://ashishthanki.github.io/model-drift-and-concept-drift"},{"title":"SHAP and Synergy","description":"SHAP and Synergy matrix will be an important part of a machine learning pipeline.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEEklEQVQ4y0WU3W+TZRjG+/cYPVET48Y2Bmu7te/3V9+v9d26FQJBUQSREIEQEoEYD2CRD8cgLAjZ6ApBYFmAaNCDGYPxwATQRaLAWrqOdd0HyLqf9u2MB3fuPPfBL8/1XNf9RF6trHCb37lWfED+xwdcn7jN+M0JRnNXOHlqkKGzwwwOnefcuWFu3hjnxlqNj0+QG7vK1G9TrALfVh7ybKZEZOnFC6SX53jn7mne/HiIVtGgvT3O+qhEtFOhI66wsVMlGhPZUJ+3xVjfFiUWTfLa62/x9fAlakDf4zwLL5eJVJcWSV24TuLuZdT5S5j9WYwuEyPlozh9qE4fupbC0B0Mw8UwvbA7TkBry0byo/kQuP3RnbBHqgtVtN3HiB87gj75OdrAJyi7tiKqPmIqg2QFyLKJolgosoWkpJBUBz2VpqmpjdxILgRt/vU7iuUykdnZWXwvTTIqInVoKDNnkX/6kngig5gKkE0fSfdRVHsNaCMaaTQ3S3NzO2MjOVaATT9PUl1eJrKwsIDfu4WkYqOYHtLOHUhDh1AeniD+6SGi9keIqV5k3UfWHESrJyzd7Wdda5Sxy/kQmJ38ZU1ytYrXs4WE4iBbaZKd3Qj7diM+Ok/nhZPEDh4l3rcD0elH0j2EOtBMo9u9NLf8D+y9c68huVKZx+vup0t2SDpZZMMl4W1hffYw0tQg0h/nad31FZ19uxDM7lC+rNqYlkdzUztjow3JmW++p7q03LihG2wmIVkouousuaEZnentxI8P0HVjGOH+BYQDexGEAFmzQ4OsOnBdR3jDV3XgyK0QHKlUKrjpTSQEHVXSUTQH2fARzDQd/QeJfTGA/OQUyuAh5K3vI2smqloH+jSt6yC3Jjk4c4Vn5Vkic3NzpNMZREENXVRkA1mxwmiIdi9JN0tS24x07wRqcQilTURKGBhuD++2xMnlrjaAAxepLi4SmZ+fJ92dQRC0RjTUVMNNoxvJDJDMNKLioex7D+3MXrRbR5D3bEMT3IbLuSuh5ODoWVZqqw3JQdCPJJloho9meChmN7KdQbWC8KxpJnqHgp7pQV25hDa0H8Pupa0jTj5/tQHcf5zZ53ONHNqWTywmkhR04rJDXPGIKy5dokFS0BCSCgnJoEu36dqgk5g4jFK7yNtvtDAyPNrY5T1HeD5Xabjs+xkSCRVJMpBUG1F1wqBLshWunSjqjQ2pP0VcRz6wHTP3GU2nP2Ds/g+sLP1Nduc+6j9XpFwuk920LXTNtjx8N8BzA6xUN64T4Lo9WKk0jt+P4/Xh+QG25BH0biW2Osw1pqjNLvHh7r0sLCwSASiVZigUihSLRf56/DjshUKBp0+nKRaKTE8XePLkKdP/zUoliuUSf84UeEWN1VqNUqnE4r8u/wO4cGdzGfQFOQAAAABJRU5ErkJggg=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/dbc86/redundancy_matrix.png","srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/491cb/redundancy_matrix.png 108w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/cce50/redundancy_matrix.png 217w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/dbc86/redundancy_matrix.png 433w","sizes":"(min-width: 433px) 433px, 100vw"},"sources":[{"srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/37a8e/redundancy_matrix.avif 108w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/3a20c/redundancy_matrix.avif 217w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/31ec6/redundancy_matrix.avif 433w","type":"image/avif","sizes":"(min-width: 433px) 433px, 100vw"},{"srcSet":"/static/9cf268fca9705dd6e4a08e4d5c2c553c/2f570/redundancy_matrix.webp 108w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/9b724/redundancy_matrix.webp 217w,\n/static/9cf268fca9705dd6e4a08e4d5c2c553c/f4857/redundancy_matrix.webp 433w","type":"image/webp","sizes":"(min-width: 433px) 433px, 100vw"}]},"width":433,"height":368},"coverImageAlt":"Redundancy Matrix produced by BCG Gamma","datePublished":"2023-02-10T00:00:00.000Z","dateModified":"2023-02-10T00:00:00.000Z","category":"Interpretability","tags":["Machine Learning"],"excerpt":"Understanding how and why a model produces an output is becoming an increasingly important stage when building a machine learning solution…","timeToRead":3,"slug":"/shap-and-synergy","route":"/shap-and-synergy","pathName":"/shap-and-synergy","url":"https://ashishthanki.github.io/shap-and-synergy"}]}},
    "staticQueryHashes": ["3661114550"]}