{
    "componentChunkName": "component---themes-advanced-src-templates-post-query-ts",
    "path": "/hyperparameter-optimization-algorithms",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Hyperparameter Optimization Algorithms\",\n  \"cover\": \"images/HPO/hpo.jpeg\",\n  \"coverAlt\": \"Hyperparameter Optimization Algorithms\",\n  \"description\": \"Hyperparameter Tuning\",\n  \"datePublished\": \"2022-07-25\",\n  \"dateModified\": \"2022-07-25\",\n  \"category\": \"Machine Learning\",\n  \"tags\": [\"Machine Learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Introduction\"), mdx(\"p\", null, \"There are many Hyperparameter Optimization (HPO) libraries to aid in finding the best hyperparameters for ML models but all of them incorporate different search algorithms, thus, outputting different parameters and model scores if the number of iterations are not sufficient.\"), mdx(\"p\", null, \"The library to use ultimately depends on the machine learning problem that you are trying to solve and complexity of the model, for example, deep neural networks have a larger number of parameters and an exhaustive search would take days (or even weeks!) and would be far from the best approach.\"), mdx(\"p\", null, \"Exhaustive methods such as grid search (and randomized grid search) evaluates all possible hyperparameter combinations (or number of defined iterations, \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"n_iters\"), \", for randomized) within the parameter grid and reports back the best combination. The issues with the exhaustive methods is that we do not use past evaluation results to select next hyperparameters to evaluate. The two most popular HPO search algorithms: Bayesian Optimization and Sequential model-based optimization (SMBO) use knowledge from previous hyperparameter combinations to select the next set.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/a3c353d33932642124fb5b0d0ac0425f/16ad0/hpo.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"38.020833333333336%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEE/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQID/9oADAMBAAIQAxAAAAHVQijU/8QAGBABAAMBAAAAAAAAAAAAAAAAAQACEwP/2gAIAQEAAQUCNRNJW3Wf/8QAFhEBAQEAAAAAAAAAAAAAAAAAABIB/9oACAEDAQE/AZxL/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAISQVH/2gAIAQIBAT8BoiuH/8QAGxAAAgEFAAAAAAAAAAAAAAAAAQIAERIhI3L/2gAIAQEABj8CItY4jLRuodZn/8QAGxABAAICAwAAAAAAAAAAAAAAAQARITFBUaH/2gAIAQEAAT8hxLHdeept6sAn2GorTuf/2gAMAwEAAgADAAAAEAwP/8QAGREBAAIDAAAAAAAAAAAAAAAAAQARIVGR/9oACAEDAQE/EDCVvkBRP//EAB0RAAEEAgMAAAAAAAAAAAAAAAEAESFhUYGh0fD/2gAIAQIBAT8QIEmlgd59FKvz2v/EABwQAQABBAMAAAAAAAAAAAAAAAERACExQWFxkf/aAAgBAQABPxCWUlxBR6L0UojaCTY6zQE3oLZBc4r/2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Illustrative diagram showing Hyperparameter tuning problem with a 2D search space from CMU ML Blog.\",\n    \"title\": \"Illustrative diagram showing Hyperparameter tuning problem with a 2D search space from CMU ML Blog.\",\n    \"src\": \"/static/a3c353d33932642124fb5b0d0ac0425f/212bf/hpo.jpg\",\n    \"srcSet\": [\"/static/a3c353d33932642124fb5b0d0ac0425f/7809d/hpo.jpg 192w\", \"/static/a3c353d33932642124fb5b0d0ac0425f/4ecad/hpo.jpg 384w\", \"/static/a3c353d33932642124fb5b0d0ac0425f/212bf/hpo.jpg 768w\", \"/static/a3c353d33932642124fb5b0d0ac0425f/16ad0/hpo.jpg 907w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Illustrative diagram showing Hyperparameter tuning problem with a 2D search space from CMU ML Blog.\"), \"\\n  \"), mdx(\"h3\", null, \"Bayesian Optimization\"), mdx(\"p\", null, \"Similar to the workflow of Bayesian statistics, by keeping track of past evaluation results, we can build a probabilistic model (called a surrogate) that can be represented as p(y | x). The surrogate model is easier to optimize than the objective function, as we are selecting hyperparameters that perform best on the surrogate function. Essentially, this method selects the next hyperparameters in an informed manner unlike grid search or random search. Spending more time in selecting the hyperparameters and fewer calls to the objective function itself \\u2013 even though the time spent choosing them are incomparable to the time spent in the objective function.\"), mdx(\"p\", null, \"The downside of the Bayesian approach is that many runs of the objective function need to be performed in order to get a representative surrogate model.\"), mdx(\"h3\", null, \"Sequential Model Based Optimization (SMBO)\"), mdx(\"p\", null, \"The Sequential model-based optimization (SMBO) methods are a formalization of Bayesian optimization. Similar to Bayesian optimization, it applies Bayesian reasoning and updates the surrogate model. The popular surrogate models could be Gaussian processes, Random Forest Regressions and Tree Parzen Estimators (TPE).\"), mdx(\"p\", null, \"Surrogate models can be thought of as probability representation of the objective function built using previous evaluation trials. The high dimensional mapping of hyperparameters is called the response surface. The SMBO method has several variations while the Bayesian Optimization approach uses a probabilistic model of the objective function and is less efficient than SMBO methods at finding the best hyperparameters. \"), mdx(\"p\", null, \"The process then uses the Expecting Improvement criteria (EI) to balance exploration versus exploitation when selecting the next hyperparameters. It creates 2 distributions where the objective function is positive and one that is negative, one distribution where the values are definitely below a threshold (score, i.e. RMSE), l(x), and one distribution where the value are definitely above a threshold, g(x). After doing a bit of maths, it turns out that the EI is proportional to this ratio (i.e. l(x) / g(x)) and maximizing it leads to a larger EI value. \"), mdx(\"p\", null, \"Using the EI, surrogate model and objective function. We evaluate hyperparameters with the greatest EI on the objective function using its result we can build an accurate surrogate model. An accurate surrogate model will then allow fewer calls to the objective function.\"), mdx(\"p\", null, \"There are many popular python libraries that use SMBO including the extremely popular: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://hyperopt.github.io/hyperopt/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"hyperopt\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://optuna.readthedocs.io/en/stable/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"optuna\"), \".\"), mdx(\"h4\", null, \"Scheduling\"), mdx(\"p\", null, \"Scheduling is an important aspect of HPO. We can save a lot of time by not following through on poor trials. Some Scheduling algorithms work extremely well with some Bayesian optimisation based libraries and SMBO libraries, the popular ones tend to be Asynchronous Successive Halving Algorithm (ASHA), Median stopping, Hyperband, population based training (PBT).\"), mdx(\"p\", null, \"All of these scheduling algorithms have there use cases and should be looked into further based on your application. For example, PBT takes its inspiration from genetic algorithms where each member of the population can exploit information from the remainder of the population. For example, a worker might copy the model parameters from a better performing worker and randomly explore new hyperparameters by changing the current values randomly.\"), mdx(\"p\", null, \"Some of the best libraries for hyperparameter tuning uses aggressive scheduling to stop poor trials quickly and use this information to select the next hyperparameter set. I would recommend diving deep into the parameters that the scheduler takes, as the default values will mostly likely not be sufficient.\"), mdx(\"p\", null, \"My recommendation would be to use \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.05934\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ASHA schedular\"), \" in majority of machine learning problems as it has been shown to outperform existing state-of-the-art hyperparameter optimization methods.\"), mdx(\"h1\", null, \"Conclusion\"), mdx(\"p\", null, \"Hyperparameter optimization libraries would select from any number of search algorithms and schedulers. Many, if not all, would offer runtime gains over exhaustive methods. A combination of Bayesian approaches and aggressive early stopping can help evaluate the optimal parameters that we are searching for, even with deep neural networks or tabular search space models. Many libraries offer asynchronous capabilities and GPU compatibility which can introduce 10x speed gains. I would recommend taking a look at \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.ray.io/en/latest/tune/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Ray Tune\"), \" HPO library. It is one of the very few HPO libraries that offers experiment execution and hyperparameter tuning at any scale while being able to tune many machine learning frameworks. \"), mdx(\"p\", null, \"Further Reading:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.sciencedirect.com/science/article/pii/S1674862X19300047\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Massive Parallel Hyperparameter Optimization\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://tech.preferred.jp/en/blog/optuna-release/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"What is Optuna blog\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://journals.sagepub.com/doi/pdf/10.1177/0020294020932347\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"New Automatic machine learning based hyperparameter optimization\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://druce.ai/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Beyond Grid Search\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Bayesian Hyperparameter Optimization blog\"))));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":3,"excerpt":"Introduction There are many Hyperparameter Optimization (HPO) libraries to aid in finding the best hyperparameters for ML models but all of…","frontmatter":{"title":"Hyperparameter Optimization Algorithms","description":"Hyperparameter Tuning","cover":{"publicURL":"/static/a3c353d33932642124fb5b0d0ac0425f/hpo.jpeg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAEDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAECA//aAAwDAQACEAMQAAAB1wE0jU//xAAYEAEBAQEBAAAAAAAAAAAAAAABAgMTI//aAAgBAQABBQL1NDoNVtL/AP/EABcRAQEBAQAAAAAAAAAAAAAAAAEAEiH/2gAIAQMBAT8ByQcv/8QAGREBAAIDAAAAAAAAAAAAAAAAAQARAgMh/9oACAECAQE/AaKhrwepP//EABsQAAEEAwAAAAAAAAAAAAAAAAEAAhEiEmGh/9oACAEBAAY/AhUmeKMSaqAx7tr/xAAaEAACAwEBAAAAAAAAAAAAAAABEQAhQTFR/9oACAEBAAE/IQaTESbqH649PfIr4GNn/9oADAMBAAIAAwAAABD7/wD/xAAZEQEAAgMAAAAAAAAAAAAAAAABACERQfH/2gAIAQMBAT8QsK28mABP/8QAHREBAAEDBQAAAAAAAAAAAAAAAQAhMUFxgaHR8P/aAAgBAgEBPxASoxyWfbwYCrr3P//EAB0QAQABAwUAAAAAAAAAAAAAAAERACFRQWGBobH/2gAIAQEAAT8QmESGAwePNJs8MoAr3Z0namoTAijpX//Z"},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/a3c353d33932642124fb5b0d0ac0425f/5d342/hpo.jpg","srcSet":"/static/a3c353d33932642124fb5b0d0ac0425f/b403d/hpo.jpg 227w,\n/static/a3c353d33932642124fb5b0d0ac0425f/3130c/hpo.jpg 454w,\n/static/a3c353d33932642124fb5b0d0ac0425f/5d342/hpo.jpg 907w","sizes":"(min-width: 907px) 907px, 100vw"},"sources":[{"srcSet":"/static/a3c353d33932642124fb5b0d0ac0425f/d2c46/hpo.avif 227w,\n/static/a3c353d33932642124fb5b0d0ac0425f/493cc/hpo.avif 454w,\n/static/a3c353d33932642124fb5b0d0ac0425f/45dfc/hpo.avif 907w","type":"image/avif","sizes":"(min-width: 907px) 907px, 100vw"},{"srcSet":"/static/a3c353d33932642124fb5b0d0ac0425f/cb8e2/hpo.webp 227w,\n/static/a3c353d33932642124fb5b0d0ac0425f/cefc8/hpo.webp 454w,\n/static/a3c353d33932642124fb5b0d0ac0425f/d845d/hpo.webp 907w","type":"image/webp","sizes":"(min-width: 907px) 907px, 100vw"}]},"width":907,"height":343}}},"coverAlt":"Hyperparameter Optimization Algorithms","datePublished":"2022-07-25","dateModified":"2022-07-25","category":"Machine Learning","tags":["Machine Learning"]},"fields":{"slug":"/hyperparameter-optimization-algorithms","route":"/hyperparameter-optimization-algorithms","pathName":"/hyperparameter-optimization-algorithms","url":"https://ashishthanki.github.io/hyperparameter-optimization-algorithms"},"internal":{"content":"---\ntitle: \"Hyperparameter Optimization Algorithms\"\ncover: images/HPO/hpo.jpeg\ncoverAlt: \"Hyperparameter Optimization Algorithms\"\ndescription: \"Hyperparameter Tuning\"\ndatePublished: \"2022-07-25\"\ndateModified: \"2022-07-25\"\ncategory: \"Machine Learning\"\ntags:\n  - Machine Learning\n---\n\n\n# Introduction\n\nThere are many Hyperparameter Optimization (HPO) libraries to aid in finding the best hyperparameters for ML models but all of them incorporate different search algorithms, thus, outputting different parameters and model scores if the number of iterations are not sufficient.\n\nThe library to use ultimately depends on the machine learning problem that you are trying to solve and complexity of the model, for example, deep neural networks have a larger number of parameters and an exhaustive search would take days (or even weeks!) and would be far from the best approach.\n\nExhaustive methods such as grid search (and randomized grid search) evaluates all possible hyperparameter combinations (or number of defined iterations, `n_iters`, for randomized) within the parameter grid and reports back the best combination. The issues with the exhaustive methods is that we do not use past evaluation results to select next hyperparameters to evaluate. The two most popular HPO search algorithms: Bayesian Optimization and Sequential model-based optimization (SMBO) use knowledge from previous hyperparameter combinations to select the next set.\n\n![Illustrative diagram showing Hyperparameter tuning problem with a 2D search space from CMU ML Blog.](images/HPO/hpo.jpeg)\n\n### Bayesian Optimization\n\nSimilar to the workflow of Bayesian statistics, by keeping track of past evaluation results, we can build a probabilistic model (called a surrogate) that can be represented as p(y | x). The surrogate model is easier to optimize than the objective function, as we are selecting hyperparameters that perform best on the surrogate function. Essentially, this method selects the next hyperparameters in an informed manner unlike grid search or random search. Spending more time in selecting the hyperparameters and fewer calls to the objective function itself – even though the time spent choosing them are incomparable to the time spent in the objective function.\n\nThe downside of the Bayesian approach is that many runs of the objective function need to be performed in order to get a representative surrogate model.\n\n### Sequential Model Based Optimization (SMBO)\n\nThe Sequential model-based optimization (SMBO) methods are a formalization of Bayesian optimization. Similar to Bayesian optimization, it applies Bayesian reasoning and updates the surrogate model. The popular surrogate models could be Gaussian processes, Random Forest Regressions and Tree Parzen Estimators (TPE).\n\nSurrogate models can be thought of as probability representation of the objective function built using previous evaluation trials. The high dimensional mapping of hyperparameters is called the response surface. The SMBO method has several variations while the Bayesian Optimization approach uses a probabilistic model of the objective function and is less efficient than SMBO methods at finding the best hyperparameters. \n\nThe process then uses the Expecting Improvement criteria (EI) to balance exploration versus exploitation when selecting the next hyperparameters. It creates 2 distributions where the objective function is positive and one that is negative, one distribution where the values are definitely below a threshold (score, i.e. RMSE), l(x), and one distribution where the value are definitely above a threshold, g(x). After doing a bit of maths, it turns out that the EI is proportional to this ratio (i.e. l(x) / g(x)) and maximizing it leads to a larger EI value. \n\nUsing the EI, surrogate model and objective function. We evaluate hyperparameters with the greatest EI on the objective function using its result we can build an accurate surrogate model. An accurate surrogate model will then allow fewer calls to the objective function.\n\nThere are many popular python libraries that use SMBO including the extremely popular: [hyperopt](http://hyperopt.github.io/hyperopt/) and [optuna](https://optuna.readthedocs.io/en/stable/index.html).\n\n\n#### Scheduling\nScheduling is an important aspect of HPO. We can save a lot of time by not following through on poor trials. Some Scheduling algorithms work extremely well with some Bayesian optimisation based libraries and SMBO libraries, the popular ones tend to be Asynchronous Successive Halving Algorithm (ASHA), Median stopping, Hyperband, population based training (PBT).\n\nAll of these scheduling algorithms have there use cases and should be looked into further based on your application. For example, PBT takes its inspiration from genetic algorithms where each member of the population can exploit information from the remainder of the population. For example, a worker might copy the model parameters from a better performing worker and randomly explore new hyperparameters by changing the current values randomly.\n\nSome of the best libraries for hyperparameter tuning uses aggressive scheduling to stop poor trials quickly and use this information to select the next hyperparameter set. I would recommend diving deep into the parameters that the scheduler takes, as the default values will mostly likely not be sufficient.\n\nMy recommendation would be to use [ASHA schedular](https://arxiv.org/abs/1810.05934) in majority of machine learning problems as it has been shown to outperform existing state-of-the-art hyperparameter optimization methods.\n\n\n# Conclusion\n\nHyperparameter optimization libraries would select from any number of search algorithms and schedulers. Many, if not all, would offer runtime gains over exhaustive methods. A combination of Bayesian approaches and aggressive early stopping can help evaluate the optimal parameters that we are searching for, even with deep neural networks or tabular search space models. Many libraries offer asynchronous capabilities and GPU compatibility which can introduce 10x speed gains. I would recommend taking a look at [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) HPO library. It is one of the very few HPO libraries that offers experiment execution and hyperparameter tuning at any scale while being able to tune many machine learning frameworks. \n\nFurther Reading:\n\n- [Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization](https://www.sciencedirect.com/science/article/pii/S1674862X19300047)\n- [Massive Parallel Hyperparameter Optimization](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n- [What is Optuna blog](https://tech.preferred.jp/en/blog/optuna-release/)\n- [New Automatic machine learning based hyperparameter optimization](https://journals.sagepub.com/doi/pdf/10.1177/0020294020932347)\n- [Beyond Grid Search](https://druce.ai/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna)\n- [Bayesian Hyperparameter Optimization blog](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n"}}},"pageContext":{"slug":"/hyperparameter-optimization-algorithms","nexttitle":"Stranger Methods","nextslug":"/stranger-methods","prevtitle":"Get Data using boto3","prevslug":"/get-data-using-boto-3","relatedPosts":[{"title":"Model Drift and Concept Drift","description":"Maintaining a good in-production model requires understanding the understanding of model and concept drift.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd1ORYA//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAQABBQKaOI//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAIDH/2gAIAQEABj8CKv8A/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAERITFRQf/aAAgBAQABPyHNYn6IdFrWX1iQoP/aAAwDAQACAAMAAAAQA8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMXFBof/aAAgBAQABPxBVtSpsKgjtDxmHQ6xbodZwxdvtBMZ7P//Z"},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg","srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/76a0b/degradation-point.jpg 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f5d63/degradation-point.jpg 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7acdb/degradation-point.jpg 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/cc266/degradation-point.jpg 1180w","sizes":"(min-width: 590px) 590px, 100vw"},"sources":[{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/8ec5f/degradation-point.avif 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/24e48/degradation-point.avif 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/23579/degradation-point.avif 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/630d8/degradation-point.avif 1180w","type":"image/avif","sizes":"(min-width: 590px) 590px, 100vw"},{"srcSet":"/static/c21aa3f5b54bbb6c20eefa63c18b5086/a168a/degradation-point.webp 148w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/6e9d7/degradation-point.webp 295w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/7b043/degradation-point.webp 590w,\n/static/c21aa3f5b54bbb6c20eefa63c18b5086/f3c07/degradation-point.webp 1180w","type":"image/webp","sizes":"(min-width: 590px) 590px, 100vw"}]},"width":590,"height":368},"coverImageAlt":"Deployed models have a degradation point and we need to avoid it!","datePublished":"2023-07-31T00:00:00.000Z","dateModified":"2023-07-31T00:00:00.000Z","category":"Ops","tags":["Machine Learning","DevOps"],"excerpt":"Model drift is a silent killer for in-production models and something that data scientists need to be mindful of. The importance of MLOps…","timeToRead":4,"slug":"/model-drift-and-concept-drift","route":"/model-drift-and-concept-drift","pathName":"/model-drift-and-concept-drift","url":"https://ashishthanki.github.io/model-drift-and-concept-drift"},{"title":"End to End Data Science Workflow","description":"The typical data science E2E workflow and pipeline.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAPoAAAD6AG1e1JrAAADbklEQVQ4y5WS21MadxTH96/qax/iTKcPaaftU6cPtlNNO5NMk3aSmo4dUzQxahHiDUEu4u4ChusKi7C4sMslK7DAEnRZQNAoqUhrq2hbE9ndXwfodKYvNj1z3s58vmfO93ugcrms0WiMRiOGYVar1eFwAABkWQZvUNDFxYVGo9FqtQqFYmRkRK1W/w/49PQ0nU5zHMcwDE3TJEm2Wi0AgCiKkiT9B7yzsxMIBBAECQaDJEnCMMyy7Bsuh+r1+hqG0RRFU5TH7fL5vKEQUSqXtnm+KAiyJF+hAlWqVTOKEiRFkNFVO+Z0rsEr6FPkqR/z4phHFMWrNle3t0jbKuXSB2zqML4UWkdcOKz3ac0+hzUYKXLV5k/HvSt69S+4wbK8zbntHGGNn+X9t2KhO0Zs/BvvV6NunWKNMk2txomOBZLYBv+QstwTg+qlUmN/r76bE7KhMufdF+KlciqcDYTYZLZ2WCvUmy9P2h1KbkvglSi3uxK9GKBCgXfNh2m0GDXzKiT6MF2Z2qw+wVbWE2P24A+28EO9+V7coiDhsah1LLigTBvNofn5hMF4UK1CR80GMhzAhjl8KHP/tue9UPZdd+pt5eQDw7XvtX3DS9fuqt7yTbxjG+rbGO8L3X1//dbXcH9/8PMBXKWCXuy+jFiEnO+Ide1b8Jxxq27KNVAS90cX1xMGX1yP4YsZfDnnNyUcc2GTNotaUgjCmM3NgwPoRS1P4I88zlHcOxELzjJ+ZYZ4vLNFdn256sM6hvHPaffygN1w0274Qj320bTi+uyj65hz4fi3k929am231mqdACBJ4iWQRSBLQOq0LIkdWJaB1PVPAuDZxkLA8rHTNLAG9yNzH6wufpjAPilwse5YEiXQ60uxg8gygLq5/51+87BSKpB8PsJzRCrhz2fCxecJB/IjQ6FMBElGYYYyx0lzkkZSMZjecECSJJ+dvz5t/XnYOG4c/dr85ezn4/Pf/2hfivLFK+ns/LV+5ubK/KBuenB57kud6sbM+Kd69Q29elA3Owzt75WYgKLKLZXTqlJyUticqqRVXusQqrvjQe97rd/B2tte67f85nQhoeSiE2x4NEc9Tm88iEfsUOPwgCEm+c15PmUSMhaBRUsZ+BkxS7gnY8EnlE9J41NJckZgTcWUXkgby1mkkoPz8Tk+H/0LpmKVQKg7XHsAAAAASUVORK5CYII="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/c4dadd7c4934e3b8fb089c0df362eade/1198f/data-science-process.webp","srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/4be21/data-science-process.webp 97w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/2061a/data-science-process.webp 194w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/1198f/data-science-process.webp 388w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/cfa69/data-science-process.webp 776w","sizes":"(min-width: 388px) 388px, 100vw"},"sources":[{"srcSet":"/static/c4dadd7c4934e3b8fb089c0df362eade/6ee32/data-science-process.avif 97w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/bfa01/data-science-process.avif 194w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/4a107/data-science-process.avif 388w,\n/static/c4dadd7c4934e3b8fb089c0df362eade/1ee87/data-science-process.avif 776w","type":"image/avif","sizes":"(min-width: 388px) 388px, 100vw"}]},"width":388,"height":368},"coverImageAlt":"E2E Pipeline","datePublished":"2023-07-15T00:00:00.000Z","dateModified":"2023-07-15T00:00:00.000Z","category":"Data Science","tags":["Machine Learning"],"excerpt":"The data science workflow can be cumbersome for beginners to handle and often some parts are forgotten completely, such as splitting the…","timeToRead":7,"slug":"/end-to-end-data-science-workflow","route":"/end-to-end-data-science-workflow","pathName":"end-to-end-data-science-workflow","url":"https://ashishthanki.github.io/end-to-end-data-science-workflow"}]}},
    "staticQueryHashes": ["3661114550"]}