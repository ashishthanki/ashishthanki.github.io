{
    "componentChunkName": "component---themes-advanced-src-templates-post-query-ts",
    "path": "/interpreting-ml-models-part-2",
    "result": {"data":{"mdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Interpreting ML Models Part 2\",\n  \"cover\": \"images/cover_6.jpeg\",\n  \"coverAlt\": \"A view of mountains.\",\n  \"description\": \"Second part in understanding ML Models.\",\n  \"datePublished\": \"2021-09-26\",\n  \"dateModified\": \"2021-09-26\",\n  \"category\": \"Interpretability\",\n  \"tags\": [\"Machine Learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Introduction\"), mdx(\"p\", null, \"Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory measures being taken, such as GDPR and the EU\\u2019s \\\"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Right_to_explanation\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Right to Explanations\"), \"\\\", alongside the rise in interest in applying machine learning. This has mandated data scientists to explain why a model has given a certain prediction. For example, institutions with highly sensitive data (i.e. personal data), that have models that output a potentially life changing decision would mandate regulations and require the data scientists to explain the decision that had been made. Hence, this blog post on model interpreting.\"), mdx(\"p\", null, \"This blog is part 2 of a 2 part series where this blog covers SHAP and LIME. The first part of this series covered  Feature Importance, Permutation Importance and Partial Dependence Plots, I recommend reading that first before this blog post, check it out \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"./interpreting-ml-models\"\n  }, \"here\"), \".\"), mdx(\"h2\", null, \"Summary of Permutation Importance and Partial Dependence\"), mdx(\"p\", null, \"By using Permutation Importance you can identify which features are important and then by using Partial Dependence plots you understand how the prediction varies based on the changes to individual features. But what if you want to know the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"impact each feature has\"), \" on one specific prediction. This is where SHAP and LIME comes in, by using these techniques you will then be able to explain why a certain predicted value was reached and have a better understanding of your machine learning models.\"), mdx(\"h2\", null, \"SHapley Additive exPlanations (SHAP)\"), mdx(\"p\", null, \"SHapley Additive exPlanations (SHAP) is a game theory approach to explain the output of any machine learning model. SHAP can break down a prediction to show the impact of each feature. The SHAP explanation requires us to compute the shapley values from coalition game theory.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"The shapley value is a solution for computing feature contributions for single predictions for any machine learning model.\")), mdx(\"p\", null, \"A shapley value is based on game theory. Each feature represents a \\u201Cplayer\\u201D in the game and the prediction represents the pay-out. The distribution of the pay-outs are shapley values. The shapley values is a method that assigns payouts to features depending on their contribution to the model's prediction. The features cooperate in a coalition and receive a certain payout because of this corporation. The shapley value is the average marginal contribution of a feature's value when making predictions. \"), mdx(\"p\", null, \"...If that didn't make sense then I would suggest learning a bit more about Game Theory, once you do that replace the word \\u201Cfeatures\\u201D with \\u201Cplayers\\u201D in the above explanation and it should click.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/a4332a312ce6528d6747f631a9471414/8079d/SHAP_values.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"71.35416666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAABXElEQVQoz52Q7UvCUBTG9/9DVBQSJWUv9KEvYWmgFkNprD74kjFnmeZGIFIqbdPtvpxzYjPDrIV0eLj3ci7Pee79KUREgCThFwFKirsJfQrFFBL5nIFgFF8Kk4J1e4FaY5oRlOr82gjUWlC6F8V6T70rPo60tqs/u3rH0Tqe3nFuu57Wdp76Y8FZlKybtJ2jZJ52C5S6or1LTOZp62J4cnNUGafK7n7ZPax6BxXvuOZlmxO17bfeePRsjD78r4qSp/55bIAEgADhPpOcrRJCx1/Alk7+RnkuK07T3EUzIswuYguRc+75ETAuhSTkUgIgR2CvI96w4OFFNCzZtIVpi4YlDEuYtjRtblhg2k6t5bke40wBdwIDBwbv0B9CbwiFKqylYTMD62ehNjOQyMLGedhMZGE1DSuncifHfQ6ICvmcxgH57PPA5FKslqWNM0I/cCjhjAV9zcbFqIXmB7/jI7/9FAskAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"SHAP values. Example SHAP value plot. Further explanation is provided below.\",\n    \"title\": \"SHAP values. Example SHAP value plot. Further explanation is provided below.\",\n    \"src\": \"/static/a4332a312ce6528d6747f631a9471414/e5715/SHAP_values.png\",\n    \"srcSet\": [\"/static/a4332a312ce6528d6747f631a9471414/8514f/SHAP_values.png 192w\", \"/static/a4332a312ce6528d6747f631a9471414/804b2/SHAP_values.png 384w\", \"/static/a4332a312ce6528d6747f631a9471414/e5715/SHAP_values.png 768w\", \"/static/a4332a312ce6528d6747f631a9471414/4ad3a/SHAP_values.png 1152w\", \"/static/a4332a312ce6528d6747f631a9471414/71c1d/SHAP_values.png 1536w\", \"/static/a4332a312ce6528d6747f631a9471414/8079d/SHAP_values.png 2100w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"SHAP values. Example SHAP value plot. Further explanation is provided below.\"), \"\\n  \"), mdx(\"p\", null, \"When a prediction is made, the summation of an instance's SHAP values for each feature explains why the prediction was different from the baseline. The baseline is the average shapley value for all predictions. If the SHAP value is large for a given feature than the contribution to the model's prediction is large. \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WARNING\"), \": The shapley value is not the prediction if we had removed a feature from the dataset. It is the average contribution of a feature value to a prediction across all possible coalitions.\"), mdx(\"p\", null, \"However, the downside of calculating shapley values is the computational load. The number of coalitions can grow exponentially based on the number of features, and the number of iterations can contribute a large amount to the computational time. We handle both of these by taking a sample of coalitions and limiting the number of iterations both of which contribute towards the variance in the final shapley value. In effect, when we use Python libraries such as \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"shap\"), \" we are using estimations. \"), mdx(\"h3\", null, \"Estimating SHAP\"), mdx(\"p\", null, \"This leads very nicely to SHAP estimations. SHAP uses efficient methods to estimate the shapley values for a given machine learning model. There are 2 popular estimations \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"TreeSHAP\"), \" and \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"KernelSHAP\"), \". \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"code\", {\n    parentName: \"li\",\n    \"className\": \"language-text\"\n  }, \"TreeSHAP\"), \" estimates SHAP values for models that are decision tree based and,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"code\", {\n    parentName: \"li\",\n    \"className\": \"language-text\"\n  }, \"KernelSHAP\"), \" is inspired by local surrogate models and can estimate other types of model but has its disadvantages.\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Computational complexity from \", mdx(\"code\", {\n    parentName: \"em\",\n    \"className\": \"language-text\"\n  }, \"KernelSHAP\"), \" to \", mdx(\"code\", {\n    parentName: \"em\",\n    \"className\": \"language-text\"\n  }, \"TreeSHAP\"), \" is reduced significantly, $O(TL^2M)$ to $O(TLD^2)$.\")), mdx(\"p\", null, \"The downside to \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"KernalSHAP\"), \" is that the kernel may be increase weightings for samples that are unrealistic. This is a common problem for permutation based interpretation methods. For example, changing either feature when a set of features are correlated, i.e. age and resting heart rate, could produce samples that are unrealistic. We would not expect a low resting heart rate high when the age is low. This will result in some features dependence being ignored.  \"), mdx(\"h2\", null, \"Types of SHAP plots\"), mdx(\"p\", null, \"The \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://shap.readthedocs.io/en/latest/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"SHAP\")), \" library is able to produce all the plots described below. It also has many other visualizations which are comparable to permutation importance and partial dependence plots.\"), mdx(\"h3\", null, \"1. Force Plots\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/8e73a32ba44d53acc655b844c895e7e4/75a80/force_plot.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"22.395833333333336%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA/0lEQVQY03WMz0oCYRxFf2/QqwVBtGvRLlO0NkG7oFWboD/atKjewGlUkiAoiMqByGhRKWmUxKQWzox+MxnfiXFctGlxOPfexRVARoRaajDRRk8hIqyZwsy6MH8gzG0KiUNhdkOY3hY/V5Bm92Oy47TSK+WOJE+Q28/oZygSKGUErl8OBgMrKFUKKm0ch6ldS2WMYpjKWSqZLYaLe5ZayBbDZNy/Ezt5b3n/fMt8vs6UOuZS/v3orOaewnBVaPccui44X/D4BlcPUHkCu4a+jLOuNuCuMXYTbuqj/f6lj90KsV8DHO8H4EI0VDUMNXga/L/wj8f0iYn2XvSmtTZ/AR07A68DmJPCAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Force Plot example. A force plot example.\",\n    \"title\": \"Force Plot example. A force plot example.\",\n    \"src\": \"/static/8e73a32ba44d53acc655b844c895e7e4/e5715/force_plot.png\",\n    \"srcSet\": [\"/static/8e73a32ba44d53acc655b844c895e7e4/8514f/force_plot.png 192w\", \"/static/8e73a32ba44d53acc655b844c895e7e4/804b2/force_plot.png 384w\", \"/static/8e73a32ba44d53acc655b844c895e7e4/e5715/force_plot.png 768w\", \"/static/8e73a32ba44d53acc655b844c895e7e4/75a80/force_plot.png 1134w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Force Plot example. A force plot example.\"), \"\\n  \"), mdx(\"p\", null, \"A force plot shows, for a single instance, which features contribute towards pushing the model to a certain value. As you can see above the the feature values shown in blue are reducing the prediction value while the feature vales shown in red are increasing the prediction. \"), mdx(\"h3\", null, \"2. Summary Plots\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/300e77af12b4646537dc7dccc103751c/ae694/SHAP_summary-plots-example.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"47.91666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABs0lEQVQoz02RW28TMRCF99fzzC9AvCEEPCCkqpQClSJaUSLRtGoaJVEvKelm2Y3t9W099oyncsID82jNd86Z4yqlxMyJsnHBGuuGITjPSBTLu7UWAJiZKZP3YMxWmIjEOWNKlXVucLYx9Ob7ZnQwv/s8jbcNdz0KU0RTEkIMw8DKTs4eP769/vppvr6qWZhkh6rrOikVM5/OzeGL43g6Y+ZMmXrPuwmhJCoLU/Xq5fnk3cWfk4VvTQ6xCiFoXUwetvDt9ZhXfwuMlPRQ0JydddY6ZpYP4uD99PzDJRufANHHarvdCiGY2QQ6+7JMO8NM2fceiYwx6/XauQJTq68OZ79+F3UMyWtfSSn3cEx0MboXNlGh2SgfYwSApmn+wczjo8XyZFG6ANLSVNZapcrNoTU/R6uQ8t7ZuVgUYxRCeF/iRMDL8VotG2aGgBCwqut67wyr7ubHXcYCE2U3IOfc9/1ms7G2FAbMN9dt3+hdixjgPzgIdz95yj7uPjVrj+U2RK31PjYz385a8VRgD+SBqhgjIgKAlLJ9rHulhBDOeQ8p72a/4JxTStZ12zad1r0bAkR8BsvvNWZcqhUHAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Summary Plots. Taken from research paper on interpreting machine learning models of biological age.\",\n    \"title\": \"Summary Plots. Taken from research paper on interpreting machine learning models of biological age.\",\n    \"src\": \"/static/300e77af12b4646537dc7dccc103751c/e5715/SHAP_summary-plots-example.png\",\n    \"srcSet\": [\"/static/300e77af12b4646537dc7dccc103751c/8514f/SHAP_summary-plots-example.png 192w\", \"/static/300e77af12b4646537dc7dccc103751c/804b2/SHAP_summary-plots-example.png 384w\", \"/static/300e77af12b4646537dc7dccc103751c/e5715/SHAP_summary-plots-example.png 768w\", \"/static/300e77af12b4646537dc7dccc103751c/ae694/SHAP_summary-plots-example.png 850w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Summary Plots. Taken from research paper on interpreting machine learning models of biological age.\"), \"\\n  \"), mdx(\"p\", null, \"We can combine multiple force plots together and then rotate it to have a summary across the entire dataset. Summary plots show many useful insights into all the features, each dot shows three characteristics: \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Vertical location is the feature it is representing.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Color shows the magnitude of the feature value for that feature. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Horizontal location is the effect of that value causing a higher or lower prediction. \")), mdx(\"p\", null, \"In the plots you can gain a strong intuition on how a decision was made by your black box model. Some features will generally have no input in the prediction until a certain feature value, whereas some features will have zero input and is ignored entirely by the model. \"), mdx(\"h3\", null, \"3. SHAP Dependence Plots\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"496px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/51da19b92568b1855140a0668638e94c/bb630/SHAP_dependence_plots.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"66.14583333333334%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACIklEQVQ4y2WTPYtTQRSG72+x9ydYWtjYWFlYiJWIWAVle7Gx0FIMYiFW2uiyyrKIKKRYQ4xxFyJsNNFsLhtzv3K/5n7MPDIzScxmD5zizJx55z3nPcdRSqEtyzLiOGYZa6ulgrUYFFIqk7N0cypKsvEUHTnLQyEEvu8jpTSxfgj2bruX8bBVMi0srFzcrNyPkZ3BaUDN0PM8A2jBYGcMlx785vz1LuduHNK494Xo4Bj8CNIMqhpECX4C30cWcFlMmqaWYVFZXmHM45vveXaxSftqk/2tt7xs7HL/zkdePDng66c/CFGRBjnSi6E3NDiOLlV7IQoDWC0++Nx4w4fn34B8Vbq24bRk56hm4NWUtbL5QYLs/iSpChxdqjuZECcJM/cE+aoFt5/y6NYu/QVIJa1A8pRA/P8oyohah7j+zDIcDUekSUKYZ3DhLh3nCu/2JlZpjaas2rrfcuHrKsdC0f4xYzZ1cZIkYT6fkyUpQTbneK/P1uXX7B9lq9Gxjxec1PrYWJZhAW23Ig09q7JWNs8F6Tyk70uuNf+aXpkRUWoD5OwcBrmiN7XjtlJZM40Cn3FYs91NUXKzX2dtmREIDWgjZ/mTBvRmM4pK8suXZ5jpKjbZWZEUfq7onGwArg93HCcEYUgURcZ1j/WcBkFg1jMMQ+P6PAoDYlEzmKZmh1Ylr69gXdcURUFZllRVZT5yXZc8zw1Tfa7z9F0uxGLfbQX/AGeZ3ejlOJhmAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"SHAP Dependence Plots. SHAP Dependence plot showing the affects of feature magnitude (age) on predicted values (marital status) and SHAP value.\",\n    \"title\": \"SHAP Dependence Plots. SHAP Dependence plot showing the affects of feature magnitude (age) on predicted values (marital status) and SHAP value.\",\n    \"src\": \"/static/51da19b92568b1855140a0668638e94c/bb630/SHAP_dependence_plots.png\",\n    \"srcSet\": [\"/static/51da19b92568b1855140a0668638e94c/8514f/SHAP_dependence_plots.png 192w\", \"/static/51da19b92568b1855140a0668638e94c/804b2/SHAP_dependence_plots.png 384w\", \"/static/51da19b92568b1855140a0668638e94c/bb630/SHAP_dependence_plots.png 496w\"],\n    \"sizes\": \"(max-width: 496px) 100vw, 496px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"SHAP Dependence Plots. SHAP Dependence plot showing the affects of feature magnitude (age) on predicted values (marital status) and SHAP value.\"), \"\\n  \"), mdx(\"p\", null, \"The SHAP dependence plots are used to identify how a feature magnitude affects the predicted value. You can then observe the general pattern (or not) on how the feature affects prediction.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"517px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/6cd20bb0ad3cf6d619e70ccd842a8b6d/fa2f5/minimal_trend_shap_dependence_plot.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"73.4375%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACDElEQVQ4y4VU246bQAzl/3+mUqW+ROo+bLvKQ6Wqz+0qDaQFEsItDMNcbE7lARKyu20tjNFczhzPsYnGcYSYxOX7LVuvexVX+6L1pJhSCpvNBtvtFk+fn/D4+AlZlv0DLDwYhgHMjGiZlIHL5QLvPZIkwcPDR7z/8A5fv32BUt1ME2DihfMNOCnQJikI4wS4TBhjQERXtowBAIHhUA4x9NDJYGC07As7v/8GKnVLeWHYthc456+LiT0sa7BlnKoaqtPwhtB3Ft7dDqbnDJd9KmchEkZN0wQAYTgBzvkB8KOGrjvo1qE7KRS7MgA67WDsBNrFZ7RZCZaUu67Dfr8PYG3bgogDIMuLGIM2OORnqIHRNxrqVw3H010STYf2cQlbKWSnHJGoKoDCUNIWxp4RvK4HpD8btFpB+2FdQ8HHWaDyUCPdZ8iPOSJRdrfbBTBhK9KzZ5BnmN6ibUyQ59T/ALGdy2S+4xkwz3vUtUaep4gk1cPhEIAWlUcewZ4w9gbEk5I80q3u5qrwfgI8ZgrnQkrO3ass6ZOniaH1YQPziJfdtHaxfWEQ53UQ8lqHwiwwZL6Vg+PA9n/2XHrU9kXrWWvR930AlW/nXIieKFwHzVHGFzfGAuyR1A7JqQFGugeURdJ6U/lQiGmaoigKxHGMqqpwPB5nMHMVUbISMne9/Lc/jNY63O/i69Z8y/4Ap+mSmPAYe44AAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Minimal Trend SHAP Dependence plot. SHAP Dependence plot showing a small trend between ball possession and goals scored. As you can see from the data, some teams record a 70% ball possession but fall short of scoring goals - which is what we would expect.\",\n    \"title\": \"Minimal Trend SHAP Dependence plot. SHAP Dependence plot showing a small trend between ball possession and goals scored. As you can see from the data, some teams record a 70% ball possession but fall short of scoring goals - which is what we would expect.\",\n    \"src\": \"/static/6cd20bb0ad3cf6d619e70ccd842a8b6d/fa2f5/minimal_trend_shap_dependence_plot.png\",\n    \"srcSet\": [\"/static/6cd20bb0ad3cf6d619e70ccd842a8b6d/8514f/minimal_trend_shap_dependence_plot.png 192w\", \"/static/6cd20bb0ad3cf6d619e70ccd842a8b6d/804b2/minimal_trend_shap_dependence_plot.png 384w\", \"/static/6cd20bb0ad3cf6d619e70ccd842a8b6d/fa2f5/minimal_trend_shap_dependence_plot.png 517w\"],\n    \"sizes\": \"(max-width: 517px) 100vw, 517px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Minimal Trend SHAP Dependence plot. SHAP Dependence plot showing a small trend between ball possession and goals scored. As you can see from the data, some teams record a 70% ball possession but fall short of scoring goals - which is what we would expect.\"), \"\\n  \"), mdx(\"p\", null, \"Conversely, if there was large standard deviation, with no apparent pattern, then we can assume that other features are interacting with this feature. Similarly, if a feature has arbitrary values then it would lead to arbitrary predictions. For example, a binary feature within the dataset of value 0 could make the feature under investigation more relevant but when the value is 1 makes the feature irrelevant. \"), mdx(\"p\", null, \"If points follow a trend but are widely spread out (in both SHAP and feature magnitude) then we can assume the permutation importance is high because the prediction is sensitive to magnitude. However, a few outliers can ruin this assumption. It is worth producing Force plots for those points to understand why they do not follow the trend.\"), mdx(\"h2\", null, \"Local Interpretable Model-Agnostic Explanations - LIME\"), mdx(\"p\", null, \"Lets reflect back on what we have learnt so far. Using \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"PI\"), \" and \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"PD\"), \" have their limitations, as discussed within the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"./Interpreting-ML-Models\"\n  }, \"Part 1\"), \" blog. SHAP are subject to incorrect interpretation because the observations from multiple plots have to be used in order to make intuitions about the features. All these interpretation methodologies work really well with simple models but can be difficult to use, and interpret, when using it on complex models. These methods depend highly on model inference time and could take a lot of computational resources. This is where LIME (Local Interpretable Model-Agnostic Explanations) comes in. \"), mdx(\"p\", null, \"LIME is a simple method that provides human-understandable model interpretation. It produces a local linear interpretable machine learning model which results in a reduced computational complexity. LIME is able to explain any complex model's prediction and allow logical intuitions about the features.\"), mdx(\"figure\", {\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"768px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/66a9ab9761fa21e8795da8afd46f0063/e8950/LIME_titanic_example.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"19.270833333333332%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsTAAALEwEAmpwYAAAArUlEQVQI112MOxIBURREZwsWY2k2gUgiUOSqLEEJlCqfgFiA8iuGwTXPfe8a824zEp8TdHUHp4PDOT4ZIYqjcxTzTUQkSZy14pz842ySsgenELmLSEBE/V53s14B8KoAos0iOuyy9o16AG4xpk4tHrY0cQACVb1u54YuCqQZj+VsGoZ7Zvsj+0eW7TIKORTzMMdM/lyrMrMx5kKkCu/9j/uek9GgXik1G1XL/JKfatDhFDFYCGAAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"LIME Titanic Example. Example of LIME interpreting a XGM model on predicting whether a passenger survived the Titanic. The model has realised that sex is an important feature.\",\n    \"title\": \"LIME Titanic Example. Example of LIME interpreting a XGM model on predicting whether a passenger survived the Titanic. The model has realised that sex is an important feature.\",\n    \"src\": \"/static/66a9ab9761fa21e8795da8afd46f0063/e5715/LIME_titanic_example.png\",\n    \"srcSet\": [\"/static/66a9ab9761fa21e8795da8afd46f0063/8514f/LIME_titanic_example.png 192w\", \"/static/66a9ab9761fa21e8795da8afd46f0063/804b2/LIME_titanic_example.png 384w\", \"/static/66a9ab9761fa21e8795da8afd46f0063/e5715/LIME_titanic_example.png 768w\", \"/static/66a9ab9761fa21e8795da8afd46f0063/4ad3a/LIME_titanic_example.png 1152w\", \"/static/66a9ab9761fa21e8795da8afd46f0063/71c1d/LIME_titanic_example.png 1536w\", \"/static/66a9ab9761fa21e8795da8afd46f0063/e8950/LIME_titanic_example.png 2000w\"],\n    \"sizes\": \"(max-width: 768px) 100vw, 768px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"LIME Titanic Example. Example of LIME interpreting a XGM model on predicting whether a passenger survived the Titanic. The model has realised that sex is an important feature.\"), \"\\n  \"), mdx(\"p\", null, \"The \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/marcotcr/lime\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"`lime'\"), \" library was created by the originators of LIME. It has a simple API that allows us to gain explanations about the model no matter how complex. You can visualize the output of the \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Explainer\"), \" objects to understand how each of the features influences the prediction - the \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Explainer\"), \" object is within the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.explanation\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"Explanation\"), \" module\"), \".\"), mdx(\"br\", null), mdx(\"h2\", null, \"Python Libraries\"), mdx(\"p\", null, \"Below are relevant links to Python libraries, and code snippets, that have been discussed throughout the 2 blog posts.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pdpbox.readthedocs.io/en/latest/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"PDPbox\")), \" helps visualize the partial dependence plots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://eli5.readthedocs.io/en/latest/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"eli5\")), \" helps debug and explain machine learning models, and helps us visualize permutation importance.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://shap.readthedocs.io/en/latest/index.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"shap\")), \" allows us to interpret models using SHAP values.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/marcotcr/lime\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, mdx(\"code\", {\n    parentName: \"a\",\n    \"className\": \"language-text\"\n  }, \"lime\")), \" allows us to use local explanations (LIME) to interpret models.\")), mdx(\"h4\", null, mdx(\"code\", {\n    parentName: \"h4\",\n    \"className\": \"language-text\"\n  }, \"pdpbox\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Calculate and show partial dependence plot:\")), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"py\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-py\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"from\"), \" matplotlib \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" pyplot \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"as\"), \" plt\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"from\"), \" pdpbox \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" pdp\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" get_dataset\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" info_plots\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# Create the data that we will plot\"), \"\\npdp_goals \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" pdp\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"pdp_isolate\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"model\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"my_model\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" dataset\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"X\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" model_features\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"feature_names\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" feature\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"'Goal Scored'\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# plot it\"), \"\\npdp\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"pdp_plot\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"pdp_goals\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"'Goal Scored'\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nplt\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"show\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"h4\", null, mdx(\"code\", {\n    parentName: \"h4\",\n    \"className\": \"language-text\"\n  }, \"eli5\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Calculate and show permutation importance:\")), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"py\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-py\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" eli5\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"from\"), \" eli5\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"sklearn \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" PermutationImportance\\n\\nperm \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" PermutationImportance\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"my_model\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" random_state\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"fit\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"X\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" y\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\neli5\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"show_weights\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"perm\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" feature_names \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" X\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"columns\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"tolist\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"h4\", null, mdx(\"code\", {\n    parentName: \"h4\",\n    \"className\": \"language-text\"\n  }, \"shap\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Calculate and show SHAP Values for One Prediction (i.e Force plot):\")), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"py\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-py\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" shap  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# package used to calculate Shap values\"), \"\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# use 1 row of data here. Could use multiple rows if desired\"), \"\\ndata_for_prediction \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" X\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"iloc\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \":\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), \"  \\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# Create object that can calculate shap values\"), \"\\nexplainer \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" shap\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"TreeExplainer\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"my_model\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nshap_values \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" explainer\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"shap_values\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"data_for_prediction\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nshap\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"initjs\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nshap\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"force_plot\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"explainer\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"expected_value\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" shap_values\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" data_for_prediction\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"h4\", null, mdx(\"code\", {\n    parentName: \"h4\",\n    \"className\": \"language-text\"\n  }, \"lime\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Calculate and show feature importance plot using LIME:\")), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"py\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-py\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"from\"), \" lime\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"lime_text \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" LimeTextExplainer \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# import Explainer object\"), \"\\nexplainer \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" LimeTextExplainer\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"class_names\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"class_names\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# create explainer object with single instance and a classifier function \"), \"\\nexp \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" explainer\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"explain_instance\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"X\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" pipeline\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"predict_proba\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" num_features\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"6\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# note: you should check how this local linear model compares\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# to your original model.\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# If the linear model is not representative of original model\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# for that region then it is not wise to use that linear \"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# model to make conclusions about features.\"), \"\\n\\nexp\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"as_list\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# list of weighted features\"), \"\\nexp\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"as_pyplot_figure\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# explanations return as a plot\")))), mdx(\"br\", null), mdx(\"h3\", null, \"When to use each?\"), mdx(\"p\", null, \"Use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"permutation importance\"), \" if you require a \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"succinct model summary\"), \". Summarizing importance of all features in a global sense. \"), mdx(\"p\", null, \"Use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"partial plot\"), \" to understand \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"individual features or relationships\"), \" between features (i.e. using 2D partial dependence plots). Summarizing the importance of a single feature and its affect on the prediction in a global sense.\"), mdx(\"p\", null, \"Use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SHAP\"), \" when you need to show the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"impact of each feature for a given row\"), \". If you require a global sense of a feature you can also use this too. \"), mdx(\"p\", null, \"Use \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LIME\"), \" when computational load is large for the above methods or if the outputs are ambiguous. This will allow you to understand your model from local points within you dataset.\"), mdx(\"p\", null, \"Shapley values are the only method that provides contrastive explanations, where it has the potential to compare predictions to a subset of data, or a single data point. Using SHAP provides a consistent approach for both global and local explanations. Although, it can be easily misinterpreted. Meanwhile, methods like LIME are very easy to interpret. However, the premise that there is a local linear relationship might not always be true.\"), mdx(\"br\", null), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"Many methods can help us identify which features to focus on and then measure individual feature importance. However, interpreting these values are crucial for data scientists to avoid introducing biases into the prediction. SHAP is widely considered the optimal solution due to its Game Theory approach, however, all interpretation methods should be explored before being confirmed by a domain expert. SHAP has the potential to be misinterpreted and can hide biases. \"), mdx(\"p\", null, \"Being able to use machine learning to predict the probability of a patient having cancer is a great asset to have for any health care professional. However, informing a patient, \\u201Cyou have a high probability of being diagnosed cancer\\u201D is not enough. Interpreting a machine learning model's prediction can break down which features a patient should focus on in order to reduce their risk.\"), mdx(\"p\", null, \"Model interpretation is not going away. Law makers will demand data scientists to interpret their models and these tools are just the start of an expanding specialism within data science. Personally, I am excited by tools like LIME, that are trying solve the limitations of its predecessors while providing even more insight about our machine learning models.\"), mdx(\"br\", null), mdx(\"h4\", null, \"Further reading:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Things to avoid when interpreting models - highly recommend reading \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/2007.04131.pdf\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"this paper by Standford\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://christophm.github.io/interpretable-ml-book/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Interpretable Machine Learning Book\"), \" by Christoph Molnar\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"PyData Talk on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=C80SQe16Rao\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Interpreting models with LIME and SHAPE\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"PyData Talk on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=0yXtdkIL3Xk\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"SHAP Values\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SHAP Dependence plots \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Notebook\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A good example of a \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.researchgate.net/publication/330144045_An_interpretable_machine_learning_model_of_biological_age\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"researchgate paper\"), \" interpreting SHAP values to provide conclusions on Biological age. \")));\n}\n;\nMDXContent.isMDXComponent = true;","timeToRead":7,"excerpt":"Introduction Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory","frontmatter":{"title":"Interpreting ML Models Part 2","description":"Second part in understanding ML Models.","cover":{"publicURL":"/static/b57872d67fc00d945468ce891f38f187/cover_6.jpeg","childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAGkhnAX/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEDERMU/9oACAEBAAEFAnM2bUdDP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CjRWYf//EABoQAAICAwAAAAAAAAAAAAAAAAABIVExYbH/2gAIAQEAAT8hzaQ7XSkq4P/aAAwDAQACAAMAAAAQDC//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEUH/2gAIAQMBAT8Q2I//xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QGl1d/8QAHBABAAICAwEAAAAAAAAAAAAAAQAhEZExUWGx/9oACAEBAAE/EDEe0jSbichdpTUAoMemXyf/2Q=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/b57872d67fc00d945468ce891f38f187/ee7c2/cover_6.jpg","srcSet":"/static/b57872d67fc00d945468ce891f38f187/74341/cover_6.jpg 625w,\n/static/b57872d67fc00d945468ce891f38f187/c98fe/cover_6.jpg 1250w,\n/static/b57872d67fc00d945468ce891f38f187/ee7c2/cover_6.jpg 2500w","sizes":"(min-width: 2500px) 2500px, 100vw"},"sources":[{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/44552/cover_6.avif 625w,\n/static/b57872d67fc00d945468ce891f38f187/fdd2c/cover_6.avif 1250w,\n/static/b57872d67fc00d945468ce891f38f187/d8633/cover_6.avif 2500w","type":"image/avif","sizes":"(min-width: 2500px) 2500px, 100vw"},{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/a07b5/cover_6.webp 625w,\n/static/b57872d67fc00d945468ce891f38f187/f1147/cover_6.webp 1250w,\n/static/b57872d67fc00d945468ce891f38f187/0676d/cover_6.webp 2500w","type":"image/webp","sizes":"(min-width: 2500px) 2500px, 100vw"}]},"width":2500,"height":1000}}},"coverAlt":"A view of mountains.","datePublished":"2021-09-26","dateModified":"2021-09-26","category":"Interpretability","tags":["Machine Learning"]},"fields":{"slug":"/interpreting-ml-models-part-2","route":"/interpreting-ml-models-part-2","pathName":"/interpreting-ml-models-part-2","url":"https://ashishthanki.github.io/interpreting-ml-models-part-2"},"internal":{"content":"---\ntitle: \"Interpreting ML Models Part 2\"\ncover: images/cover_6.jpeg\ncoverAlt: \"A view of mountains.\"\ndescription: \"Second part in understanding ML Models.\"\ndatePublished: \"2021-09-26\"\ndateModified: \"2021-09-26\"\ncategory: \"Interpretability\"\ntags:\n  - Machine Learning\n---\n\n<!-- markdownlint-disable-->\n\n\n# Introduction\n\nExplaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory measures being taken, such as GDPR and the EUs \"[Right to Explanations](https://en.wikipedia.org/wiki/Right_to_explanation)\", alongside the rise in interest in applying machine learning. This has mandated data scientists to explain why a model has given a certain prediction. For example, institutions with highly sensitive data (i.e. personal data), that have models that output a potentially life changing decision would mandate regulations and require the data scientists to explain the decision that had been made. Hence, this blog post on model interpreting.\n\nThis blog is part 2 of a 2 part series where this blog covers SHAP and LIME. The first part of this series covered  Feature Importance, Permutation Importance and Partial Dependence Plots, I recommend reading that first before this blog post, check it out [here](./interpreting-ml-models).\n\n## Summary of Permutation Importance and Partial Dependence\n\nBy using Permutation Importance you can identify which features are important and then by using Partial Dependence plots you understand how the prediction varies based on the changes to individual features. But what if you want to know the *impact each feature has* on one specific prediction. This is where SHAP and LIME comes in, by using these techniques you will then be able to explain why a certain predicted value was reached and have a better understanding of your machine learning models.\n\n## SHapley Additive exPlanations (SHAP)\n\nSHapley Additive exPlanations (SHAP) is a game theory approach to explain the output of any machine learning model. SHAP can break down a prediction to show the impact of each feature. The SHAP explanation requires us to compute the shapley values from coalition game theory.\n\n> The shapley value is a solution for computing feature contributions for single predictions for any machine learning model.\n\nA shapley value is based on game theory. Each feature represents a player in the game and the prediction represents the pay-out. The distribution of the pay-outs are shapley values. The shapley values is a method that assigns payouts to features depending on their contribution to the model's prediction. The features cooperate in a coalition and receive a certain payout because of this corporation. The shapley value is the average marginal contribution of a feature's value when making predictions. \n\n...If that didn't make sense then I would suggest learning a bit more about Game Theory, once you do that replace the word features with players in the above explanation and it should click.\n\n\n![SHAP values. Example SHAP value plot. Further explanation is provided below.](./images/Interpreting-ML-Models-Part2/SHAP_values.png)\n\n\nWhen a prediction is made, the summation of an instance's SHAP values for each feature explains why the prediction was different from the baseline. The baseline is the average shapley value for all predictions. If the SHAP value is large for a given feature than the contribution to the model's prediction is large. \n\n\n**WARNING**: The shapley value is not the prediction if we had removed a feature from the dataset. It is the average contribution of a feature value to a prediction across all possible coalitions.\n\nHowever, the downside of calculating shapley values is the computational load. The number of coalitions can grow exponentially based on the number of features, and the number of iterations can contribute a large amount to the computational time. We handle both of these by taking a sample of coalitions and limiting the number of iterations both of which contribute towards the variance in the final shapley value. In effect, when we use Python libraries such as `shap` we are using estimations. \n\n\n### Estimating SHAP \n\nThis leads very nicely to SHAP estimations. SHAP uses efficient methods to estimate the shapley values for a given machine learning model. There are 2 popular estimations `TreeSHAP` and `KernelSHAP`. \n\n- `TreeSHAP` estimates SHAP values for models that are decision tree based and,\n- `KernelSHAP` is inspired by local surrogate models and can estimate other types of model but has its disadvantages.\n\n*Computational complexity from `KernelSHAP` to `TreeSHAP` is reduced significantly, $O(TL^2M)$ to $O(TLD^2)$.*\n\nThe downside to `KernalSHAP` is that the kernel may be increase weightings for samples that are unrealistic. This is a common problem for permutation based interpretation methods. For example, changing either feature when a set of features are correlated, i.e. age and resting heart rate, could produce samples that are unrealistic. We would not expect a low resting heart rate high when the age is low. This will result in some features dependence being ignored.  \n\n## Types of SHAP plots\n\nThe [`SHAP`](https://shap.readthedocs.io/en/latest/index.html) library is able to produce all the plots described below. It also has many other visualizations which are comparable to permutation importance and partial dependence plots.\n\n\n### 1. Force Plots\n\n![Force Plot example. A force plot example.](./images/Interpreting-ML-Models-Part2/force_plot.png)\n\n\nA force plot shows, for a single instance, which features contribute towards pushing the model to a certain value. As you can see above the the feature values shown in blue are reducing the prediction value while the feature vales shown in red are increasing the prediction. \n\n### 2. Summary Plots\n\n![Summary Plots. Taken from research paper on interpreting machine learning models of biological age.](./images/Interpreting-ML-Models-Part2/SHAP_summary-plots-example.png)\n\nWe can combine multiple force plots together and then rotate it to have a summary across the entire dataset. Summary plots show many useful insights into all the features, each dot shows three characteristics: \n\n- Vertical location is the feature it is representing.\n- Color shows the magnitude of the feature value for that feature. \n- Horizontal location is the effect of that value causing a higher or lower prediction. \n\n\nIn the plots you can gain a strong intuition on how a decision was made by your black box model. Some features will generally have no input in the prediction until a certain feature value, whereas some features will have zero input and is ignored entirely by the model. \n\n\n### 3. SHAP Dependence Plots\n\n![SHAP Dependence Plots. SHAP Dependence plot showing the affects of feature magnitude (age) on predicted values (marital status) and SHAP value.](./images/Interpreting-ML-Models-Part2/SHAP_dependence_plots.png)\n\nThe SHAP dependence plots are used to identify how a feature magnitude affects the predicted value. You can then observe the general pattern (or not) on how the feature affects prediction.\n\n![Minimal Trend SHAP Dependence plot. SHAP Dependence plot showing a small trend between ball possession and goals scored. As you can see from the data, some teams record a 70% ball possession but fall short of scoring goals - which is what we would expect.](./images/Interpreting-ML-Models-Part2/minimal_trend_shap_dependence_plot.png)\n\nConversely, if there was large standard deviation, with no apparent pattern, then we can assume that other features are interacting with this feature. Similarly, if a feature has arbitrary values then it would lead to arbitrary predictions. For example, a binary feature within the dataset of value 0 could make the feature under investigation more relevant but when the value is 1 makes the feature irrelevant. \n\nIf points follow a trend but are widely spread out (in both SHAP and feature magnitude) then we can assume the permutation importance is high because the prediction is sensitive to magnitude. However, a few outliers can ruin this assumption. It is worth producing Force plots for those points to understand why they do not follow the trend.\n \n## Local Interpretable Model-Agnostic Explanations - LIME\n\nLets reflect back on what we have learnt so far. Using `PI` and `PD` have their limitations, as discussed within the [Part 1](./Interpreting-ML-Models) blog. SHAP are subject to incorrect interpretation because the observations from multiple plots have to be used in order to make intuitions about the features. All these interpretation methodologies work really well with simple models but can be difficult to use, and interpret, when using it on complex models. These methods depend highly on model inference time and could take a lot of computational resources. This is where LIME (Local Interpretable Model-Agnostic Explanations) comes in. \n\nLIME is a simple method that provides human-understandable model interpretation. It produces a local linear interpretable machine learning model which results in a reduced computational complexity. LIME is able to explain any complex model's prediction and allow logical intuitions about the features.\n\n![LIME Titanic Example. Example of LIME interpreting a XGM model on predicting whether a passenger survived the Titanic. The model has realised that sex is an important feature.](./images/Interpreting-ML-Models-Part2/LIME_titanic_example.png)\n\nThe [`lime'](https://github.com/marcotcr/lime) library was created by the originators of LIME. It has a simple API that allows us to gain explanations about the model no matter how complex. You can visualize the output of the `Explainer` objects to understand how each of the features influences the prediction - the `Explainer` object is within the [`Explanation` module](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.explanation).\n\n<br/>\n\n\n\n## Python Libraries\n\nBelow are relevant links to Python libraries, and code snippets, that have been discussed throughout the 2 blog posts.\n\n- [`PDPbox`](https://pdpbox.readthedocs.io/en/latest/) helps visualize the partial dependence plots.\n- [`eli5`](https://eli5.readthedocs.io/en/latest/index.html) helps debug and explain machine learning models, and helps us visualize permutation importance.\n- [`shap`](https://shap.readthedocs.io/en/latest/index.html) allows us to interpret models using SHAP values.\n- [`lime`](https://github.com/marcotcr/lime) allows us to use local explanations (LIME) to interpret models.\n\n#### `pdpbox`\n\n**Calculate and show partial dependence plot:**\n\n```py\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=my_model, dataset=X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()\n```\n\n#### `eli5`\n\n**Calculate and show permutation importance:**\n\n```py\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(X, y)\neli5.show_weights(perm, feature_names = X.columns.tolist())\n```\n\n#### `shap`\n\n**Calculate and show SHAP Values for One Prediction (i.e Force plot):**\n\n```py \nimport shap  # package used to calculate Shap values\n\n# use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction = X.iloc[0,:]  \n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)\n```\n\n\n#### `lime`\n\n**Calculate and show feature importance plot using LIME:**\n\n```py \nfrom lime.lime_text import LimeTextExplainer # import Explainer object\nexplainer = LimeTextExplainer(class_names=class_names)\n\n# create explainer object with single instance and a classifier function \nexp = explainer.explain_instance(X[0], pipeline.predict_proba, num_features=6) \n\n# note: you should check how this local linear model compares\n# to your original model.\n# If the linear model is not representative of original model\n# for that region then it is not wise to use that linear \n# model to make conclusions about features.\n\nexp.as_list() # list of weighted features\nexp.as_pyplot_figure() # explanations return as a plot\n```\n\n<br/>\n\n### When to use each?\n\nUse **permutation importance** if you require a **succinct model summary**. Summarizing importance of all features in a global sense. \n\nUse **partial plot** to understand **individual features or relationships** between features (i.e. using 2D partial dependence plots). Summarizing the importance of a single feature and its affect on the prediction in a global sense.\n\nUse **SHAP** when you need to show the **impact of each feature for a given row**. If you require a global sense of a feature you can also use this too. \n\nUse **LIME** when computational load is large for the above methods or if the outputs are ambiguous. This will allow you to understand your model from local points within you dataset.\n\nShapley values are the only method that provides contrastive explanations, where it has the potential to compare predictions to a subset of data, or a single data point. Using SHAP provides a consistent approach for both global and local explanations. Although, it can be easily misinterpreted. Meanwhile, methods like LIME are very easy to interpret. However, the premise that there is a local linear relationship might not always be true.\n\n<br/>\n\n## Conclusion\n\nMany methods can help us identify which features to focus on and then measure individual feature importance. However, interpreting these values are crucial for data scientists to avoid introducing biases into the prediction. SHAP is widely considered the optimal solution due to its Game Theory approach, however, all interpretation methods should be explored before being confirmed by a domain expert. SHAP has the potential to be misinterpreted and can hide biases. \n\nBeing able to use machine learning to predict the probability of a patient having cancer is a great asset to have for any health care professional. However, informing a patient, you have a high probability of being diagnosed cancer is not enough. Interpreting a machine learning model's prediction can break down which features a patient should focus on in order to reduce their risk.\n\n\nModel interpretation is not going away. Law makers will demand data scientists to interpret their models and these tools are just the start of an expanding specialism within data science. Personally, I am excited by tools like LIME, that are trying solve the limitations of its predecessors while providing even more insight about our machine learning models.\n\n<br/>\n\n#### Further reading:\n\n- Things to avoid when interpreting models - highly recommend reading [this paper by Standford](https://arxiv.org/pdf/2007.04131.pdf)\n- [Interpretable Machine Learning Book](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar\n- PyData Talk on [Interpreting models with LIME and SHAPE](https://www.youtube.com/watch?v=C80SQe16Rao)\n- PyData Talk on [SHAP Values](https://www.youtube.com/watch?v=0yXtdkIL3Xk)\n- SHAP Dependence plots [Notebook](https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html)\n- A good example of a [researchgate paper](https://www.researchgate.net/publication/330144045_An_interpretable_machine_learning_model_of_biological_age) interpreting SHAP values to provide conclusions on Biological age. \n"}}},"pageContext":{"slug":"/interpreting-ml-models-part-2","nexttitle":"Interpreting ML Models","nextslug":"/interpreting-ml-models","prevtitle":"Pandas Efficient Programming","prevslug":"/pandas-efficient-programming","relatedPosts":[{"title":"Interpreting ML Models Part 2","description":"Second part in understanding ML Models.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAGkhnAX/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEDERMU/9oACAEBAAEFAnM2bUdDP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CjRWYf//EABoQAAICAwAAAAAAAAAAAAAAAAABIVExYbH/2gAIAQEAAT8hzaQ7XSkq4P/aAAwDAQACAAMAAAAQDC//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEUH/2gAIAQMBAT8Q2I//xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QGl1d/8QAHBABAAICAwEAAAAAAAAAAAAAAQAhEZExUWGx/9oACAEBAAE/EDEe0jSbichdpTUAoMemXyf/2Q=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg","srcSet":"/static/b57872d67fc00d945468ce891f38f187/2ab11/cover_6.jpg 230w,\n/static/b57872d67fc00d945468ce891f38f187/16c5c/cover_6.jpg 460w,\n/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg 920w,\n/static/b57872d67fc00d945468ce891f38f187/3f865/cover_6.jpg 1840w","sizes":"(min-width: 920px) 920px, 100vw"},"sources":[{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/ee20b/cover_6.avif 230w,\n/static/b57872d67fc00d945468ce891f38f187/5c720/cover_6.avif 460w,\n/static/b57872d67fc00d945468ce891f38f187/991c3/cover_6.avif 920w,\n/static/b57872d67fc00d945468ce891f38f187/b7eae/cover_6.avif 1840w","type":"image/avif","sizes":"(min-width: 920px) 920px, 100vw"},{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/1d6f2/cover_6.webp 230w,\n/static/b57872d67fc00d945468ce891f38f187/f0b6f/cover_6.webp 460w,\n/static/b57872d67fc00d945468ce891f38f187/d54e9/cover_6.webp 920w,\n/static/b57872d67fc00d945468ce891f38f187/382d3/cover_6.webp 1840w","type":"image/webp","sizes":"(min-width: 920px) 920px, 100vw"}]},"width":920,"height":368},"coverImageAlt":"A view of mountains.","datePublished":"2021-09-26T00:00:00.000Z","dateModified":"2021-09-26T00:00:00.000Z","category":"Interpretability","tags":["Machine Learning"],"excerpt":"Introduction Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory","timeToRead":7,"slug":"/interpreting-ml-models-part-2","route":"/interpreting-ml-models-part-2","pathName":"/interpreting-ml-models-part-2","url":"https://ashishthanki.github.io/interpreting-ml-models-part-2"},{"title":"Interpreting ML Models","description":"First part in understanding ML models.","coverImg":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAGkhnAX/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAEDERMU/9oACAEBAAEFAnM2bUdDP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CjRWYf//EABoQAAICAwAAAAAAAAAAAAAAAAABIVExYbH/2gAIAQEAAT8hzaQ7XSkq4P/aAAwDAQACAAMAAAAQDC//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEUH/2gAIAQMBAT8Q2I//xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QGl1d/8QAHBABAAICAwEAAAAAAAAAAAAAAQAhEZExUWGx/9oACAEBAAE/EDEe0jSbichdpTUAoMemXyf/2Q=="},"backgroundColor":"transparent","images":{"fallback":{"src":"/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg","srcSet":"/static/b57872d67fc00d945468ce891f38f187/2ab11/cover_6.jpg 230w,\n/static/b57872d67fc00d945468ce891f38f187/16c5c/cover_6.jpg 460w,\n/static/b57872d67fc00d945468ce891f38f187/d43de/cover_6.jpg 920w,\n/static/b57872d67fc00d945468ce891f38f187/3f865/cover_6.jpg 1840w","sizes":"(min-width: 920px) 920px, 100vw"},"sources":[{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/ee20b/cover_6.avif 230w,\n/static/b57872d67fc00d945468ce891f38f187/5c720/cover_6.avif 460w,\n/static/b57872d67fc00d945468ce891f38f187/991c3/cover_6.avif 920w,\n/static/b57872d67fc00d945468ce891f38f187/b7eae/cover_6.avif 1840w","type":"image/avif","sizes":"(min-width: 920px) 920px, 100vw"},{"srcSet":"/static/b57872d67fc00d945468ce891f38f187/1d6f2/cover_6.webp 230w,\n/static/b57872d67fc00d945468ce891f38f187/f0b6f/cover_6.webp 460w,\n/static/b57872d67fc00d945468ce891f38f187/d54e9/cover_6.webp 920w,\n/static/b57872d67fc00d945468ce891f38f187/382d3/cover_6.webp 1840w","type":"image/webp","sizes":"(min-width: 920px) 920px, 100vw"}]},"width":920,"height":368},"coverImageAlt":"A view of mountains","datePublished":"2021-07-02T00:00:00.000Z","dateModified":"2021-07-02T00:00:00.000Z","category":"Interpretability","tags":["Machine Learning"],"excerpt":"Introduction Explaining features and interpreting your models has taken a sharp rise in Europe. Partly because of new laws and regulatory","timeToRead":5,"slug":"/interpreting-ml-models","route":"/interpreting-ml-models","pathName":"interpreting-ml-models","url":"https://ashishthanki.github.io/interpreting-ml-models"}]}},
    "staticQueryHashes": ["3661114550"]}